{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "## Table of Contents\n",
    "- [Transformers](#transformers)\n",
    "  - [Self Attention](#self-attention)\n",
    "    - [Self Attention Step 1](#self-attention-step-01)\n",
    "      - [Exercise 1](#exercise-01)\n",
    "    - [Self Attention Step 2](#self-attention-step-02)\n",
    "      - [Exercise 2](#exercise-02)\n",
    "    - [Self Attention Step 3](#self-attention-step-03)\n",
    "      - [Exercise 3](#exercise-03)\n",
    "      - [Exercise 4](#exercise-04)\n",
    "    - [Self Attention Step 4](#self-attention-step-04)\n",
    "      - [Exercise 5](#exercise-05)\n",
    "    - [Self Attention All Steps](#self-attention-all-steps)\n",
    "      - [Exercise 6](#exercise-06)\n",
    "      - [Exercise 7](#exercise-07)\n",
    "  - [Multihead Self Attention](#multihead-self-attention)\n",
    "    - [Exercise 8](#exercise-08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"transformers\"></a>\n",
    "## Transformers\n",
    "\n",
    "Since it's inventory in 2017, the transformer architecture continues to be the dominat model for nearly all NLP tasks. The core idea behind the Transformer model is the self-attention mechanism which is what this notebook will concentrate on. \n",
    "\n",
    "NLP-based transformer models first process text data into vectors which are then fed into the model for further processing. This is illustrated in the first figure below\n",
    "\n",
    "![Preprocessing](../images/transformer_positional_encoding_vectors.png)\n",
    "\n",
    "Next the vectors are fed into the decoder layer, which is made of mainly a self-attention layer and feed forward layer.\n",
    "\n",
    "![Decoder Block](../images/decoder_with_tensors_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"self-attention\"></a>\n",
    "## Self-Attention\n",
    "\n",
    "The _self-attention_ block within the transformer architecture, takes $N$ inputs: ${\\boldsymbol x_{1}},\\, {\\boldsymbol x_{2}}, \\dots, {\\boldsymbol x_{N}}$, each of dimension $D \\times 1$ and returns $N$ output vectors. The operation in stepwise illustration is as follows:\n",
    "\n",
    "<a id=\"self-attention-step-01\"></a>\n",
    "### Self-Attention: Step 1 \n",
    "For each input word ${\\boldsymbol x}$, we create a _query_ vector, ${\\boldsymbol q}$, a _key_ vector, ${\\boldsymbol k}$ and a _value_ vector, ${\\boldsymbol v}$. We'll also need the weight _matrices_ and bias _vectors_ to compute the _queries_, _keys_, and _values_. In particular, \n",
    "- the weight and bias for computing the _queries_, denoted by ${\\boldsymbol W_{q}}$ and ${\\boldsymbol b_{q}}$,\n",
    "- the weight and bias for computing the _keys_, denoted by ${\\boldsymbol W_{k}}$ and ${\\boldsymbol b_{k}}$, and \n",
    "- the weight and bias for computing the _values_, denoted by ${\\boldsymbol W_{v}}$ and ${\\boldsymbol b_{v}}$\n",
    "\n",
    "As an exercise, you will now implement the computation of the _queries_, _keys_, and _values_ in python using either [torch](https://pytorch.org/) library. In the next two cells, we generate the input vectors, ${\\boldsymbol x_{1}}$, ${\\boldsymbol x_{2}}$, and ${\\boldsymbol x_{3}}$, each of dimension $D \\times 1$, with $D=4$, and the weight matrices and bias vectors ${\\boldsymbol W_{q}}$ and ${\\boldsymbol b_{q}}$ for queries, ${\\boldsymbol W_{k}}$ and ${\\boldsymbol b_{k}}$ for the keys, and ${\\boldsymbol W_{v}}$ and ${\\boldsymbol b_{v}}$ for the values. Your execise is to use these the compute the _queries_, _keys_, and _values_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector x_1 is: \n",
      " tensor([[ 0.8033],\n",
      "        [ 0.1748],\n",
      "        [ 0.0890],\n",
      "        [-0.6137]]) \n",
      "\n",
      "The vector x_2 is: \n",
      " tensor([[ 0.0462],\n",
      "        [-1.3683],\n",
      "        [ 0.3375],\n",
      "        [ 1.0111]]) \n",
      "\n",
      "The vector x_3 is: \n",
      " tensor([[-1.4352],\n",
      "        [ 0.9774],\n",
      "        [ 0.5220],\n",
      "        [ 1.2379]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set seed so we get the same random numbers\n",
    "torch.manual_seed(3)\n",
    "\n",
    "# Number of inputs\n",
    "N = 3\n",
    "\n",
    "# Number of dimensions of each input\n",
    "D = 4\n",
    "\n",
    "# Create an empty list\n",
    "all_x = []\n",
    "# Create elements x_n and append to list\n",
    "for n in range(N):\n",
    "    # Creates a tensor of shape (D, 1) with values drawn from the standard normal distribution, \n",
    "    # with a mean of 0 and a standard deviation of 1 \n",
    "    x = torch.randn(size=(D, 1))  # D x 1 tensor\n",
    "    print(f\"The vector x_{n+1} is: \\n {x} \\n\")\n",
    "    all_x.append(x)  # Append x to list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the cell above, the list object, `all_x` now has $N$ vectors, each of dimension $D \\times 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed so we get the same random numbers\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Choose random values for the parameters\n",
    "\n",
    "# weight matrices\n",
    "W_q = torch.randn(size=(D, D))\n",
    "W_k = torch.randn(size=(D, D))\n",
    "W_v = torch.randn(size=(D, D))\n",
    "\n",
    "# bais terms\n",
    "b_q = torch.randn(size=(D, 1))\n",
    "b_k = torch.randn(size=(D, 1))\n",
    "b_v = torch.randn(size=(D, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exercise-01\"></a>\n",
    "#### Exercise 1\n",
    "Given vectors: ${\\boldsymbol x}_{1}$, ${\\boldsymbol x}_{2}$, and ${\\boldsymbol x}_{3}$ in the list object **`all_x`**, and the weight matrices: ${\\boldsymbol W_{q}}$, ${\\boldsymbol W_{k}}$ and ${\\boldsymbol W_{v}}$ and bais vectors: ${\\boldsymbol b_{q}}$, ${\\boldsymbol b_{k}}$ and ${\\boldsymbol b_{v}}$, compute the query vectors: ${\\boldsymbol q_{1}}$, ${\\boldsymbol q_{2}}$, ${\\boldsymbol q_{3}}$, the key vectors: ${\\boldsymbol k_{1}}$, ${\\boldsymbol k_{2}}$, ${\\boldsymbol k_{3}}$ and the value vectors: ${\\boldsymbol v_{1}}$, ${\\boldsymbol v_{2}}$, ${\\boldsymbol v_{3}}$. \n",
    "\n",
    "Hint: \n",
    "- ${\\boldsymbol q} = {\\boldsymbol b} + {\\boldsymbol W}\\, {\\boldsymbol x}$\n",
    "- ${\\boldsymbol k} = {\\boldsymbol b} + {\\boldsymbol W}\\, {\\boldsymbol x}$\n",
    "- ${\\boldsymbol v} = {\\boldsymbol b} + {\\boldsymbol W}\\, {\\boldsymbol x}$\n",
    "\n",
    "You may want to consider using one of the following functions:\n",
    "- [torch.matmul](https://pytorch.org/docs/stable/generated/torch.matmul.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make three lists to store queries, keys, and values\n",
    "all_queries = []\n",
    "all_keys = []\n",
    "all_values = []\n",
    "\n",
    "# For every input\n",
    "for x in all_x:\n",
    "    # Compute the keys, queries, and values using PyTorch operations\n",
    "    query =  torch.matmul(W_q, x) + b_q\n",
    "    key = torch.matmul(W_k, x) + b_k\n",
    "    value = torch.matmul(W_v, x) + b_v\n",
    "\n",
    "    # Append the results to the lists\n",
    "    all_queries.append(query)\n",
    "    all_keys.append(key)\n",
    "    all_values.append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Self Attention Mechanism Step 1](../images/self_attention_step_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"self-attention-step-02\"></a>\n",
    "### Self Attention: Step 2\n",
    "\n",
    "The second step in calculating self-attention is to calculate a score. Consider the figure above, with input words _\"Thinking Machine\"_. Suppose we want to calculate the _self-attention_ for the first word, _\"Thinking\"_. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.\n",
    "\n",
    "The score is calculated by taking the **dot product** of the _query_ vector with all the _key_ vectors of the respective word we are scoring. So if we are processing the self-attention for the word in the first position, the first score would be the **dot product** of ${\\boldsymbol q_{1}}$ and ${\\boldsymbol k_{1}}$. The second score would be the dot product of ${\\boldsymbol q_{1}}$ and ${\\boldsymbol k_{2}}$. As shown in Figure 2 below: \n",
    "\n",
    "![Self Attention Mechanism Step 2](../images/self_attention_step_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exercise-02\"> </a>\n",
    "#### Exercise 2\n",
    "\n",
    "Compute the self-attention scores of the input vectors ${\\boldsymbol x_{1}}$, ${\\boldsymbol x_{2}}$, ${\\boldsymbol x_{3}}$, using their query vectors ${\\boldsymbol q_{1}}$, ${\\boldsymbol q_{2}}$, ${\\boldsymbol q_{3}}$ and key vectors: ${\\boldsymbol k_{1}}$, ${\\boldsymbol k_{2}}$, ${\\boldsymbol k_{3}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(-2.8869), tensor(3.6531), tensor(-1.1920)],\n",
       " [tensor(0.0760), tensor(4.2115), tensor(4.1211)],\n",
       " [tensor(0.8152), tensor(-6.1015), tensor(1.6231)]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "all_attention_scores = []\n",
    "for query in all_queries:\n",
    "    query_keys_attention_scores = []\n",
    "    for key in all_keys:\n",
    "        # Compute the dot product of the query and key\n",
    "        # for numerical stability you want to dot product with the square root of D\n",
    "        dot_product = torch.matmul(query.T, key).squeeze() \n",
    "        scaled_dot_product = dot_product / math.sqrt(D) \n",
    "\n",
    "        # Append the result to the list\n",
    "        query_keys_attention_scores.append(scaled_dot_product)\n",
    "    all_attention_scores.append(query_keys_attention_scores)\n",
    "    \n",
    "all_attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"self-attention-step-03\"></a>\n",
    "### Self Attention: Step 3\n",
    "\n",
    "Next we will normalise the each score in the list object `all_attention_scores` so that they are all positive and add up to $1$. We can achieve this using the [softmax function](https://en.wikipedia.org/wiki/Softmax_function), $\\sigma: \\mathbb{R}^{m} \\to (0, 1)^{m}$, defined as follows: \n",
    "\n",
    "Given the vectors ${\\boldsymbol z} = (z_{1},\\, z_{2},\\,\\dots,\\, z_{m}) \\in \\mathbb{R}^{m}$, \n",
    "\n",
    "$$\\sigma({\\boldsymbol z})_{i} = \\dfrac{e^{z_{i}}}{\\sum_{j=1}^{m} e^{z_{j}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exercise-03\"></a>\n",
    "#### Exercise 3\n",
    "Implment the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(all_queries[0], torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(items_in):\n",
    "    if not isinstance(items_in, torch.Tensor):\n",
    "        items_in = torch.Tensor(items_in)\n",
    "    # Shift the input for numerical stability (optional, but recommended)\n",
    "    # items_in = items_in - torch.max(items_in)\n",
    "\n",
    "    # Compute the exponential of the input\n",
    "    exp_items = torch.exp(items_in)\n",
    "    \n",
    "    # Compute the softmax by dividing by the sum of exponentials\n",
    "    items_out = exp_items / torch.sum(exp_items)\n",
    "\n",
    "    return items_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now use your `softmax` implementation to compute the self-attention weights by applying the softmax function to the self-attention scores you computed in <a href=\"#exercise-02\">Exercise 2</a>. Note that the self-attention scores for each input vector: ${\\boldsymbol x_{1}}$, ${\\boldsymbol x_{2}}$, ${\\boldsymbol x_{3}}$, are the contained in the list object `all_attention_scores`.\n",
    "\n",
    "<a id=\"exercise-04\"></a>\n",
    "#### Exercise 4\n",
    "Compute the self-attention weights of the input vectors: ${\\boldsymbol x_{1}}$, ${\\boldsymbol x_{2}}$, and ${\\boldsymbol x_{3}}$ using their self-attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The attention probabilities for input vector x_1 are: tensor([0.0014, 0.9908, 0.0078])\n",
      "The attention probabilities for input vector x_2 are: tensor([0.0083, 0.5183, 0.4735])\n",
      "The attention probabilities for input vector x_3 are: tensor([3.0824e-01, 3.0549e-04, 6.9145e-01])\n"
     ]
    }
   ],
   "source": [
    "all_attention_weights = []\n",
    "for idx, attention_scores in enumerate(all_attention_scores):\n",
    "    attention_weights = softmax(attention_scores)\n",
    "    print(f\"The attention probabilities for input vector x_{idx+1} are: {attention_weights}\")\n",
    "    all_attention_weights.append(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"self-attention-step-04\"></a>\n",
    "### Self Attention: Step 4\n",
    "\n",
    "The fourth step is to multiply each value vector, i.e., the values contained in the list object **`all_values`**, by their corresponding attention weights, i.e, the values contained in the list object **`all_attention_weights`**. The intuition behind this multiplication is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words, that is, multiplying irrelevant words by tiny numbers like $0.001$, which in this case are the attention weights. \n",
    "\n",
    "After multiplying the attention weights by their corresponding value vectors we sum them up. \n",
    "\n",
    "<a id=\"exercise-05\"></a>\n",
    "#### Exercise 5\n",
    "Use the attention weights you computed in <a href=\"#exercise-04\">Exercise 4</a>, i.e., the contents of **`all_attention_weights`**, and the value vectors, **`all_values`** to compute the weighted sum of all values ${\\boldsymbol v_{1}}, {\\boldsymbol v_{2}}, {\\boldsymbol v_{3}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.2117],\n",
       "         [ 1.0697],\n",
       "         [-3.3355],\n",
       "         [-4.9260]]),\n",
       " tensor([[ 0.6486],\n",
       "         [ 0.9883],\n",
       "         [-2.4109],\n",
       "         [-3.0185]]),\n",
       " tensor([[ 0.6463],\n",
       "         [ 0.8405],\n",
       "         [-1.6421],\n",
       "         [-0.0805]])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_attention_weighted_values = []\n",
    "\n",
    "# Loop over each set of attention weights\n",
    "for attention_weights in all_attention_weights:\n",
    "    attention_weighted_values = []\n",
    "    \n",
    "    # For each attention weight and corresponding value\n",
    "    for attention_weight, value in zip(attention_weights, all_values):\n",
    "        # Compute the weighted value using element-wise multiplication\n",
    "        weighted_value = attention_weight * value\n",
    "        \n",
    "        attention_weighted_values.append(weighted_value)\n",
    "    \n",
    "    # Sum the weighted values across all values\n",
    "    attention_weight_across_all_values = torch.sum(torch.stack(attention_weighted_values), dim=0)\n",
    "    \n",
    "    # Append the final result to the list\n",
    "    all_attention_weighted_values.append(attention_weight_across_all_values)\n",
    "\n",
    "all_attention_weighted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"self-attention-all-steps\"></a>\n",
    "### Self Attention All Steps\n",
    "The figure below \n",
    "\n",
    "![Complete Attention Steps](../images/self-attention-output.png)\n",
    "\n",
    "demonstrate the complete self-attention mechanism. Now let's put everything together in <a href=\"#exercise-06\"> Exercise 6</a> below\n",
    "\n",
    "<a id=\"exercise-06\"></a>\n",
    "#### Exercise 6\n",
    "\n",
    "Implement the complete self attention mechanism by completing the incomplete code snippet below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attentions for output  0\n",
      "tensor([0.0014, 0.9908, 0.0078])\n",
      "Attentions for output  1\n",
      "tensor([0.0083, 0.5183, 0.4735])\n",
      "Attentions for output  2\n",
      "tensor([3.0824e-01, 3.0549e-04, 6.9145e-01])\n",
      "\n",
      "x_prime_0_calculated: tensor([[ 0.2117,  1.0697, -3.3355, -4.9260]])\n",
      "x_prime_0_true:       tensor([[ 0.2117,  1.0697, -3.3355, -4.9260]])\n",
      "\n",
      "x_prime_1_calculated: tensor([[ 0.6486,  0.9883, -2.4109, -3.0185]])\n",
      "x_prime_1_true:       tensor([[ 0.6486,  0.9883, -2.4109, -3.0185]])\n",
      "\n",
      "x_prime_2_calculated: tensor([[ 0.6463,  0.8405, -1.6421, -0.0805]])\n",
      "x_prime_2_true:       tensor([[ 0.6463,  0.8405, -1.6421, -0.0805]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create empty list for output\n",
    "all_x_prime = []\n",
    "\n",
    "# Assuming N is defined\n",
    "for n in range(N):\n",
    "    # Create list for dot products of query N with all keys\n",
    "    all_km_qn = []\n",
    "\n",
    "    # Compute the dot products\n",
    "    for key in all_keys:\n",
    "        # Compute the dot product of the query and the key and normalize by sqrt(D)\n",
    "        dot_product = torch.matmul(key.T, all_queries[n]).squeeze() / torch.sqrt(torch.tensor(D, dtype=torch.float32))\n",
    "\n",
    "        # Store dot product\n",
    "        all_km_qn.append(dot_product)\n",
    "\n",
    "    # Convert dot products to a tensor\n",
    "    all_km_qn = torch.tensor(all_km_qn)\n",
    "\n",
    "    # Compute softmax over dot products to get attention\n",
    "    attention = softmax(all_km_qn)\n",
    "\n",
    "    # Print result (should be positive and sum to one)\n",
    "    print(\"Attentions for output \", n)\n",
    "    print(attention)\n",
    "\n",
    "    # Compute the weighted sum of all of the values according to the attention\n",
    "    x_prime = torch.sum(torch.stack([attention[i] * all_values[i] for i in range(len(all_values))]), dim=0)\n",
    "\n",
    "    # Append the result to the output list\n",
    "    all_x_prime.append(x_prime)\n",
    "\n",
    "# Print out true values to check you have it correct\n",
    "print(\"\\nx_prime_0_calculated:\", all_x_prime[0].T)\n",
    "print(\"x_prime_0_true:       tensor([[ 0.2117,  1.0697, -3.3355, -4.9260]])\\n\")\n",
    "print(\"x_prime_1_calculated:\", all_x_prime[1].T)\n",
    "print(\"x_prime_1_true:       tensor([[ 0.6486,  0.9883, -2.4109, -3.0185]])\\n\")\n",
    "print(\"x_prime_2_calculated:\", all_x_prime[2].T)\n",
    "print(\"x_prime_2_true:       tensor([[ 0.6463,  0.8405, -1.6421, -0.0805]])\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have observed, all the computations we have done, could be done using matrices, as illustrated in Figures  \n",
    "\n",
    "![Self Attention Matrix Calculation Step 1](../images/self-attention-matrix-calculation.png)\n",
    "\n",
    "This complete the first step, the remain steps are illustrated in the figure below: \n",
    "\n",
    "![Self Attention Matrix Calculation Step 2](../images/self-attention-matrix-calculation-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compute the attention scores by multiplying the set of queries packed in matrix $Q$ with the keys in the matrix $K$. If the matrix $Q$ is of size $m \\times d_k$, and the matrix $K$ is of size $n \\times d_k$, then the resulting matrix will be of size $m \\times n$:\n",
    "\n",
    "$$\n",
    "QK^\\top = \\begin{bmatrix}\n",
    "e_{11} & e_{12} & \\dots & e_{1n} \\\\\n",
    "e_{21} & e_{22} & \\dots & e_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "e_{m1} & e_{m2} & \\dots & e_{mn}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "2. Scale each of the alignment scores by $\\frac{1}{\\sqrt{d_k}}$:\n",
    "\n",
    "$$\n",
    "\\frac{QK^\\top}{\\sqrt{d_k}} = \\begin{bmatrix}\n",
    "\\frac{e_{11}}{\\sqrt{d_k}} & \\frac{e_{12}}{\\sqrt{d_k}} & \\dots & \\frac{e_{1n}}{\\sqrt{d_k}} \\\\\n",
    "\\frac{e_{21}}{\\sqrt{d_k}} & \\frac{e_{22}}{\\sqrt{d_k}} & \\dots & \\frac{e_{2n}}{\\sqrt{d_k}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{e_{m1}}{\\sqrt{d_k}} & \\frac{e_{m2}}{\\sqrt{d_k}} & \\dots & \\frac{e_{mn}}{\\sqrt{d_k}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3. And follow the scaling process by applying a softmax operation in order to obtain a set of attention weights:\n",
    "\n",
    "$$\n",
    "\\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) = \\begin{bmatrix}\n",
    "\\text{softmax}\\left( \\frac{e_{11}}{\\sqrt{d_k}} \\right) & \\text{softmax}\\left( \\frac{e_{12}}{\\sqrt{d_k}} \\right) & \\dots & \\text{softmax}\\left( \\frac{e_{1n}}{\\sqrt{d_k}} \\right) \\\\\n",
    "\\text{softmax}\\left( \\frac{e_{21}}{\\sqrt{d_k}} \\right) & \\text{softmax}\\left( \\frac{e_{22}}{\\sqrt{d_k}} \\right) & \\dots & \\text{softmax}\\left( \\frac{e_{2n}}{\\sqrt{d_k}} \\right) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{softmax}\\left( \\frac{e_{m1}}{\\sqrt{d_k}} \\right) & \\text{softmax}\\left( \\frac{e_{m2}}{\\sqrt{d_k}} \\right) & \\dots & \\text{softmax}\\left( \\frac{e_{mn}}{\\sqrt{d_k}} \\right)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "4. Finally, apply the resulting attention weights to the values in matrix $V$, of size $n \\times d_v$:\n",
    "\n",
    "$$\n",
    "\\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) \\cdot V = \n",
    "\\begin{bmatrix}\n",
    "\\text{softmax}\\left( \\frac{e_{11}}{\\sqrt{d_k}} \\right) & \\text{softmax}\\left( \\frac{e_{12}}{\\sqrt{d_k}} \\right) & \\dots & \\text{softmax}\\left( \\frac{e_{1n}}{\\sqrt{d_k}} \\right) \\\\\n",
    "\\text{softmax}\\left( \\frac{e_{21}}{\\sqrt{d_k}} \\right) & \\text{softmax}\\left( \\frac{e_{22}}{\\sqrt{d_k}} \\right) & \\dots & \\text{softmax}\\left( \\frac{e_{2n}}{\\sqrt{d_k}} \\right) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{softmax}\\left( \\frac{e_{m1}}{\\sqrt{d_k}} \\right) & \\text{softmax}\\left( \\frac{e_{m2}}{\\sqrt{d_k}} \\right) & \\dots & \\text{softmax}\\left( \\frac{e_{mn}}{\\sqrt{d_k}} \\right)\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "v_{11} & v_{12} & \\dots & v_{1d_v} \\\\\n",
    "v_{21} & v_{22} & \\dots & v_{2d_v} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{n1} & v_{n2} & \\dots & v_{nd_v}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In <a href=\"#exercise-07\">Exercise 7</a> below, you will use matrices to implement the self-attention mechanism, given the input matrix ${\\boldsymbol X}$. We provide the helper function `softmax_cols` that will be necessary as well as the input matrix ${\\boldsymbol X}$, whose columns are the vectors in **`all_x`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8033,  0.0462, -1.4352],\n",
       "        [ 0.1748, -1.3683,  0.9774],\n",
       "        [ 0.0890,  0.3375,  0.5220],\n",
       "        [-0.6137,  1.0111,  1.2379]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy data into a matrix\n",
    "X = torch.zeros((D, N))  # Create a tensor of shape (D, N) filled with zeros\n",
    "\n",
    "# Copy the data from all_x into the matrix\n",
    "X[:, 0] = all_x[0].squeeze()\n",
    "X[:, 1] = all_x[1].squeeze()\n",
    "X[:, 2] = all_x[2].squeeze()\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define softmax operation that works independently on each column\n",
    "def softmax_cols(data_in):\n",
    "    # Exponentiate all of the values\n",
    "    exp_values = torch.exp(data_in)\n",
    "    \n",
    "    # Sum over columns (dim=0 for column-wise summation)\n",
    "    denom = torch.sum(exp_values, dim=0)\n",
    "    \n",
    "    # Replicate denominator to match the input size\n",
    "    denom = denom.unsqueeze(0).expand_as(data_in)\n",
    "    \n",
    "    # Compute softmax\n",
    "    softmax = exp_values / denom\n",
    "    \n",
    "    # Return the result\n",
    "    return softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exercise-07\"></a>\n",
    "#### Exercise 7\n",
    "Compute the self-attention mechanism in matrix form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scaled dot product self-attention function\n",
    "def scaled_dot_product_self_attention(X, W_v, W_q, W_k, b_v, b_q, b_k):\n",
    "    \n",
    "    # 1. Compute queries, keys, and values using matrix multiplication and addition\n",
    "    queries = torch.matmul(W_q, X) + b_q\n",
    "    keys = torch.matmul(W_k, X) + b_k\n",
    "    values = torch.matmul(W_v, X) + b_v\n",
    "\n",
    "    # 2. Compute dot products of keys and queries (keys.T * queries)\n",
    "    attention_scores = torch.matmul(keys.T, queries)\n",
    "\n",
    "    # 3. Scale the dot products by the square root of the dimensionality of the keys\n",
    "    d_k = torch.tensor(keys.shape[0], dtype=torch.float32)  # dimensionality of the keys\n",
    "    scaled_attention_scores = attention_scores / torch.sqrt(d_k)\n",
    "\n",
    "    # 4. Apply softmax to calculate attention weights (column-wise softmax)\n",
    "    attention_weights = softmax_cols(scaled_attention_scores)\n",
    "\n",
    "    # 5. Weight the values by attention weights\n",
    "    X_prime = torch.matmul(values, attention_weights)\n",
    "\n",
    "    return X_prime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2117,  0.6486,  0.6463],\n",
       "        [ 1.0697,  0.9883,  0.8405],\n",
       "        [-3.3355, -2.4109, -1.6421],\n",
       "        [-4.9260, -3.0185, -0.0805]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prime = scaled_dot_product_self_attention(X, W_v, W_q, W_k, b_v, b_q, b_k)\n",
    "X_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"multihead-self-attention\"></a>\n",
    "## Multihead Self-Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multihead self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n} \\in \\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$. In other words, it the repetition of the self-attention a fix number of times in parallel. Self-attention occurs in parallel across multiple \"heads\". Each head has its own queries, keys, and values. The Figure below gives an illustration of a 2-head self attention, in the cyan and orange boxes, respectively. The outputs are vertically concatenated an another linear transformation layer, ${\\boldsymbol \\Omega_{c}}$ is used to recombine them.\n",
    "\n",
    "![Two Head Self Attention](../images/two-head-self-attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.077,  0.360, -0.782,  0.072,  0.665, -0.287],\n",
      "        [ 1.621, -1.597, -0.052, -0.306,  0.249, -0.223],\n",
      "        [ 0.913,  0.204,  0.574,  0.416,  0.262,  0.931],\n",
      "        [-0.514, -1.652,  1.046,  0.522, -0.167,  0.053],\n",
      "        [ 0.564,  2.257,  1.869, -1.195,  0.998,  0.459],\n",
      "        [ 2.436, -0.147, -0.476, -0.293, -0.348,  0.349],\n",
      "        [ 0.037, -0.068,  0.429, -0.868, -0.271,  0.142],\n",
      "        [ 0.130,  0.681, -0.958,  0.064,  0.659,  0.819]])\n"
     ]
    }
   ],
   "source": [
    "# Set seed so we get the same random numbers\n",
    "torch.manual_seed(3)\n",
    "\n",
    "# Number of inputs\n",
    "N = 6\n",
    "\n",
    "# Number of dimensions of each input\n",
    "D = 8\n",
    "\n",
    "# Create a tensor with random normal values (mean=0, std=1)\n",
    "mat_X = torch.randn(D, N)\n",
    "\n",
    "# Print X\n",
    "print(mat_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the Figure we will use $2$ heads. We need the weights matrices and biases vectors for the keys, queries, and values. We'll make the queries keys and values of size dimensions $\\frac{D}{H} \\times N$, as shown in the Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 2\n",
    "# QKV dimension\n",
    "H_D = int(D / H)\n",
    "\n",
    "# Set seed so we get the same random numbers\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Choose random values for the parameters for the first head\n",
    "W_q1 = torch.randn(size=(H_D, D))\n",
    "W_k1 = torch.randn(size=(H_D, D))\n",
    "W_v1 = torch.randn(size=(H_D, D))\n",
    "b_q1 = torch.randn(size=(H_D, 1))\n",
    "b_k1 = torch.randn(size=(H_D, 1))\n",
    "b_v1 = torch.randn(size=(H_D, 1))\n",
    "\n",
    "# Choose random values for the parameters for the second head\n",
    "W_q2 = torch.randn(size=(H_D, D))\n",
    "W_k2 = torch.randn(size=(H_D, D))\n",
    "W_v2 = torch.randn(size=(H_D, D))\n",
    "b_q2 = torch.randn(size=(H_D, 1))\n",
    "b_k2 = torch.randn(size=(H_D, 1))\n",
    "b_v2 = torch.randn(size=(H_D, 1))\n",
    "\n",
    "# Choose random values for the parameters\n",
    "W_c = torch.randn(size=(D, D)) # Linear transformation used to combine the vertically concatenated attention heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, a helper fuction `multi_head_softmax_cols` is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define softmax operation that works independently on each column\n",
    "def multi_head_softmax_cols(data_in):\n",
    "    # Exponentiate all of the values\n",
    "    exp_values = torch.exp(data_in)\n",
    "    \n",
    "    # Sum over columns (dim=0 for column-wise summation)\n",
    "    denom = torch.sum(exp_values, dim=0, keepdim=True)\n",
    "    \n",
    "    # Compute softmax (PyTorch broadcasts the denominator to all rows automatically)\n",
    "    softmax = exp_values / denom\n",
    "    \n",
    "    # Return the result\n",
    "    return softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exercise-08\"></a>\n",
    "#### Exercise 8\n",
    "\n",
    "Implement the multihead self-attention mechanism by completing the code snippets below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the multi-head scaled self-attention mechanism\n",
    "def multihead_scaled_self_attention(\n",
    "        X, W_v1, W_q1, W_k1, b_v1, b_q1, b_k1, W_v2, W_q2, W_k2, b_v2, b_q2, b_k2, W_c\n",
    "    ):\n",
    "    \n",
    "    # 1. Compute queries, key, and value for Head 1\n",
    "    Q1 = torch.matmul(W_q1, X) + b_q1\n",
    "    K1 = torch.matmul(W_k1, X) + b_k1\n",
    "    V1 = torch.matmul(W_v1, X) + b_v1\n",
    "\n",
    "    # 2. Compute queries, key, and value for Head 2\n",
    "    Q2 = torch.matmul(W_q2, X) + b_q2\n",
    "    K2 = torch.matmul(W_k2, X) + b_k2\n",
    "    V2 = torch.matmul(W_v2, X) + b_v2\n",
    "\n",
    "    # 3. Compute dot products\n",
    "    dot_products1 = torch.matmul(K1.T, Q1)\n",
    "    dot_products2 = torch.matmul(K2.T, Q2)\n",
    "\n",
    "    d_k = torch.tensor(K1.shape[0], dtype=torch.float32)  # dimensionality of the keys (same for both heads)\n",
    "\n",
    "    # 4. Scale dot products\n",
    "    scaled_dot_products1 = dot_products1 / torch.sqrt(d_k)\n",
    "    scaled_dot_products2 = dot_products2 / torch.sqrt(d_k)\n",
    "\n",
    "    # 5. Apply softmax to calculate attention scores\n",
    "    attentions1 = multi_head_softmax_cols(scaled_dot_products1)\n",
    "    attentions2 = multi_head_softmax_cols(scaled_dot_products2)\n",
    "\n",
    "    # 6. Weight values by attention weights\n",
    "    head1_output = torch.matmul(V1, attentions1)\n",
    "    head2_output = torch.matmul(V2, attentions2)\n",
    "\n",
    "    # 7. Concatenate the outputs of the two heads\n",
    "    concatenated_output = torch.cat((head1_output, head2_output), dim=0)\n",
    "\n",
    "    # 8. Apply the final linear transformation\n",
    "    X_prime = torch.matmul(W_c, concatenated_output)\n",
    "\n",
    "    return X_prime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your answer:\n",
      "tensor([[  7.501,  15.386,  12.121,  23.458,   5.546,  -7.499],\n",
      "        [  4.221,   4.875,  -2.205,   4.050,  -4.525,   5.155],\n",
      "        [  1.891,   3.035,   3.399,   2.733,   2.958,  -0.824],\n",
      "        [  2.621,   2.177,  -4.974,  -0.925,  -1.928,   3.726],\n",
      "        [ -0.130,  -0.250,   3.700,   0.948,   9.384,   0.697],\n",
      "        [  2.524,   1.555,  -0.789,   2.667,  -0.459,   4.428],\n",
      "        [  0.056,  -1.688,  -1.537,  -1.700,   0.391,   4.648],\n",
      "        [ -1.352,  -4.136,  -8.878,  -1.003, -12.857,  -4.945]])\n",
      "\n",
      "True values:\n",
      "tensor([[  7.501,  15.386,  12.121,  23.458,   5.546,  -7.499],\n",
      "        [  4.221,   4.875,  -2.205,   4.050,  -4.525,   5.155],\n",
      "        [  1.891,   3.035,   3.399,   2.733,   2.958,  -0.824],\n",
      "        [  2.621,   2.177,  -4.974,  -0.925,  -1.928,   3.726],\n",
      "        [ -0.130,  -0.250,   3.700,   0.948,   9.384,   0.697],\n",
      "        [  2.524,   1.555,  -0.789,   2.667,  -0.459,   4.428],\n",
      "        [  0.056,  -1.688,  -1.537,  -1.700,   0.391,   4.648],\n",
      "        [ -1.352,  -4.136,  -8.878,  -1.003, -12.857,  -4.945]])\n"
     ]
    }
   ],
   "source": [
    "X_prime = multihead_scaled_self_attention(\n",
    "    X=mat_X,\n",
    "    W_v1=W_v1,\n",
    "    W_q1=W_q1, \n",
    "    W_k1=W_k1,\n",
    "    b_v1=b_v1,\n",
    "    b_q1=b_q1,\n",
    "    b_k1=b_k1,\n",
    "    W_v2=W_v2,\n",
    "    W_q2=W_q2,\n",
    "    W_k2=W_k2,\n",
    "    b_v2=b_v2,\n",
    "    b_q2=b_q2,\n",
    "    b_k2=b_k2,\n",
    "    W_c=W_c\n",
    ")\n",
    "\n",
    "# Set precision for printing\n",
    "# torch.set_printoptions(precision=3)\n",
    "\n",
    "# Print out the results\n",
    "print(\"Your answer:\")\n",
    "print(X_prime)\n",
    "\n",
    "print(\"\\nTrue values:\")\n",
    "true_values = torch.tensor([[  7.501,  15.386,  12.121,  23.458,   5.546,  -7.499],\n",
    "                            [  4.221,   4.875,  -2.205,   4.050,  -4.525,   5.155],\n",
    "                            [  1.891,   3.035,   3.399,   2.733,   2.958,  -0.824],\n",
    "                            [  2.621,   2.177,  -4.974,  -0.925,  -1.928,   3.726],\n",
    "                            [ -0.130,  -0.250,   3.700,   0.948,   9.384,   0.697],\n",
    "                            [  2.524,   1.555,  -0.789,   2.667,  -0.459,   4.428],\n",
    "                            [  0.056,  -1.688,  -1.537,  -1.700,   0.391,   4.648],\n",
    "                            [ -1.352,  -4.136,  -8.878,  -1.003, -12.857,  -4.945]])\n",
    "\n",
    "print(true_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
