{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<a id=\"toc\"></a>\n",
    "## Table of Contents\n",
    "- [Transformers](#transformers)\n",
    "  - [Self Attention](#self-attention)\n",
    "    - [Self Attention Step 1](#self-attention-step-01)\n",
    "      - [Exercise 1](#exercise-01)\n",
    "    - [Self Attention Step 2](#self-attention-step-02)\n",
    "      - [Exercise 2](#exercise-02)\n",
    "    - [Self Attention Step 3](#self-attention-step-03)\n",
    "      - [Exercise 3](#exercise-03)\n",
    "      - [Exercise 4](#exercise-04)\n",
    "    - [Self Attention Step 4](#self-attention-step-04)\n",
    "      - [Exercise 5](#exercise-05)\n",
    "    - [Self Attention All Steps](#self-attention-all-steps)\n",
    "      - [Exercise 6](#exercise-06)\n",
    "      - [Exercise 7](#exercise-07)\n",
    "  - [Multihead Self Attention](#multihead-self-attention)\n",
    "    - [Exercise 8](#exercise-08)\n",
    "  - [References and Acknowledgements](#references-and-acknowledgement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<a id=\"transformers\"></a>\n",
    "## Transformers\n",
    "\n",
    "Since it's inventory in 2017, the transformer architecture continues to be the dominat model for nearly all NLP tasks. The core idea behind the Transformer model is the self-attention mechanism which is what this notebook will concentrate on. \n",
    "\n",
    "NLP-based transformer models first process text data into vectors which are then fed into the model for further processing. This is illustrated in the first figure below\n",
    "\n",
    "![Preprocessing](../images/transformer_positional_encoding_vectors.png)\n",
    "\n",
    "Next the vectors are fed into the decoder layer, which is made of mainly a self-attention layer and feed forward layer.\n",
    "\n",
    "![Decoder Block](../images/decoder_with_tensors_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"self-attention\"></a>\n",
    "## Self-Attention\n",
    "\n",
    "The _self-attention_ block within the transformer architecture, takes $N$ inputs: ${\\boldsymbol x_{1}},\\, {\\boldsymbol x_{2}}, \\dots, {\\boldsymbol x_{N}}$, each of dimension $D \\times 1$ and returns $N$ output vectors. The operation in stepwise illustration is as follows:\n",
    "\n",
    "<a id=\"self-attention-step-01\"></a>\n",
    "### Self-Attention: Step 1 \n",
    "For each input word ${\\boldsymbol x}$, we create a _query_ vector, ${\\boldsymbol q}$, a _key_ vector, ${\\boldsymbol k}$ and a _value_ vector, ${\\boldsymbol v}$. We'll also need the weight _matrices_ and bias _vectors_ to compute the _queries_, _keys_, and _values_. In particular, \n",
    "- the weight and bias for computing the _queries_, denoted by ${\\boldsymbol W_{q}}$ and ${\\boldsymbol b_{q}}$,\n",
    "- the weight and bias for computing the _keys_, denoted by ${\\boldsymbol W_{k}}$ and ${\\boldsymbol b_{k}}$, and \n",
    "- the weight and bias for computing the _values_, denoted by ${\\boldsymbol W_{v}}$ and ${\\boldsymbol b_{v}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Implement the computation of _queries_, _keys_, and _values_ using Python and [torch](https://pytorch.org/).\n",
    "- Generate input vectors ${\\boldsymbol x_{1}}$, ${\\boldsymbol x_{2}}$, and ${\\boldsymbol x_{3}}$ with dimensions $D \\times 1$, with $D=4$.\n",
    "- Define weight matrices and biases: \n",
    "  - ${\\boldsymbol W_{q}}, {\\boldsymbol b_{q}}$ for queries\n",
    "  - ${\\boldsymbol W_{k}}, {\\boldsymbol b_{k}}$ for keys\n",
    "  - ${\\boldsymbol W_{v}}, {\\boldsymbol b_{v}}$ for values\n",
    "- Compute the _queries_, _keys_, and _values_ using these weights and biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set seed so we get the same random numbers\n",
    "torch.manual_seed(3)\n",
    "\n",
    "# Number of inputs\n",
    "N = 3\n",
    "\n",
    "# Number of dimensions of each input\n",
    "D = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create an empty list\n",
    "all_x = []\n",
    "# Create elements x_n and append to list\n",
    "for n in range(N):\n",
    "    # Creates a tensor of shape (D, 1) with values drawn from the standard normal distribution, \n",
    "    # with a mean of 0 and a standard deviation of 1 \n",
    "    x = torch.randn(size=(D, 1))  # D x 1 tensor\n",
    "    # print(f\"The vector x_{n+1} is: \\n {x} \\n\")\n",
    "    all_x.append(x)  # Append x to list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "After executing the cell above, the list object, `all_x` now has $N$ vectors, each of dimension $D \\times 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Set seed so we get the same random numbers\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Choose random values for the parameters\n",
    "\n",
    "# weight matrices\n",
    "W_q = torch.randn(size=(D, D))\n",
    "W_k = torch.randn(size=(D, D))\n",
    "W_v = torch.randn(size=(D, D))\n",
    "\n",
    "# bais terms\n",
    "b_q = torch.randn(size=(D, 1))\n",
    "b_k = torch.randn(size=(D, 1))\n",
    "b_v = torch.randn(size=(D, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<a id=\"exercise-01\"></a>\n",
    "#### Exercise 1\n",
    "Given vectors: ${\\boldsymbol x}_{1}$, ${\\boldsymbol x}_{2}$, and ${\\boldsymbol x}_{3}$ in the list object **`all_x`**, and the weight matrices: ${\\boldsymbol W_{q}}$, ${\\boldsymbol W_{k}}$ and ${\\boldsymbol W_{v}}$ and bais vectors: ${\\boldsymbol b_{q}}$, ${\\boldsymbol b_{k}}$ and ${\\boldsymbol b_{v}}$, compute the query vectors: ${\\boldsymbol q_{1}}$, ${\\boldsymbol q_{2}}$, ${\\boldsymbol q_{3}}$, the key vectors: ${\\boldsymbol k_{1}}$, ${\\boldsymbol k_{2}}$, ${\\boldsymbol k_{3}}$ and the value vectors: ${\\boldsymbol v_{1}}$, ${\\boldsymbol v_{2}}$, ${\\boldsymbol v_{3}}$. \n",
    "\n",
    "Hint: \n",
    "- ${\\boldsymbol q} = {\\boldsymbol b} + {\\boldsymbol W}\\, {\\boldsymbol x}$\n",
    "- ${\\boldsymbol k} = {\\boldsymbol b} + {\\boldsymbol W}\\, {\\boldsymbol x}$\n",
    "- ${\\boldsymbol v} = {\\boldsymbol b} + {\\boldsymbol W}\\, {\\boldsymbol x}$\n",
    "\n",
    "You may want to consider using one of the following functions:\n",
    "- [torch.matmul](https://pytorch.org/docs/stable/generated/torch.matmul.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Make three lists to store queries, keys, and values\n",
    "all_queries = []\n",
    "all_keys = []\n",
    "all_values = []\n",
    "\n",
    "# For every input\n",
    "for x in all_x:\n",
    "    # TODO\n",
    "    # Compute query, key, and value\n",
    "    query = None\n",
    "    key = None\n",
    "    value = None\n",
    "\n",
    "    # Append the results to the lists\n",
    "    all_queries.append(query)\n",
    "    all_keys.append(key)\n",
    "    all_values.append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![Self Attention Mechanism Step 1](../images/self_attention_step_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<a id=\"self-attention-step-02\"></a>\n",
    "### Self Attention: Step 2\n",
    "\n",
    "The second step in calculating self-attention is to calculate a score. Consider the figure above, with input words _\"Thinking Machine\"_. Suppose we want to calculate the _self-attention_ for the first word, _\"Thinking\"_. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.\n",
    "\n",
    "The score is calculated by taking the **dot product** of the _query_ vector with all the _key_ vectors of the respective word we are scoring. So if we are processing the self-attention for the word in the first position, the first score would be the **dot product** of ${\\boldsymbol q_{1}}$ and ${\\boldsymbol k_{1}}$. The second score would be the dot product of ${\\boldsymbol q_{1}}$ and ${\\boldsymbol k_{2}}$. As shown in Figure 2 below: \n",
    "\n",
    "![Self Attention Mechanism Step 2](../images/self_attention_step_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<a id=\"exercise-02\"> </a>\n",
    "#### Exercise 2\n",
    "\n",
    "Compute the self-attention scores of the input vectors ${\\boldsymbol x_{1}}$, ${\\boldsymbol x_{2}}$, ${\\boldsymbol x_{3}}$, using their query vectors ${\\boldsymbol q_{1}}$, ${\\boldsymbol q_{2}}$, ${\\boldsymbol q_{3}}$ and key vectors: ${\\boldsymbol k_{1}}$, ${\\boldsymbol k_{2}}$, ${\\boldsymbol k_{3}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "all_attention_scores = []\n",
    "for query in all_queries:\n",
    "    query_keys_attention_scores = []\n",
    "    for key in all_keys:\n",
    "        # Compute the dot product of the query and key\n",
    "        # for numerical stability you want to dot product with the square root of D\n",
    "        # TODO\n",
    "        # Compute the dot product\n",
    "        dot_product = None\n",
    "        # TODO\n",
    "        # Compute the scaled dot product (Normalizing the dot product by the square root of D)\n",
    "        scaled_dot_product = None\n",
    "\n",
    "        # Append the result to the list\n",
    "        query_keys_attention_scores.append(scaled_dot_product)\n",
    "    all_attention_scores.append(query_keys_attention_scores)\n",
    "    \n",
    "all_attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Your output should be\n",
    "```python\n",
    "[[tensor(-2.8869), tensor(3.6531), tensor(-1.1920)],\n",
    " [tensor(0.0760), tensor(4.2115), tensor(4.1211)],\n",
    " [tensor(0.8152), tensor(-6.1015), tensor(1.6231)]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<a id=\"self-attention-step-03\"></a>\n",
    "### Self Attention: Step 3\n",
    "\n",
    "Next we will normalise the each score in the list object `all_attention_scores` so that they are all positive and add up to $1$. We can achieve this using the [softmax function](https://en.wikipedia.org/wiki/Softmax_function), $\\sigma: \\mathbb{R}^{m} \\to (0, 1)^{m}$, defined as follows: \n",
    "\n",
    "Given the vectors ${\\boldsymbol z} = (z_{1},\\, z_{2},\\,\\dots,\\, z_{m}) \\in \\mathbb{R}^{m}$, \n",
    "\n",
    "$$\\sigma({\\boldsymbol z})_{i} = \\dfrac{e^{z_{i}}}{\\sum_{j=1}^{m} e^{z_{j}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<a id=\"exercise-03\"></a>\n",
    "#### Exercise 3\n",
    "Implement the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(items_in):\n",
    "    if not isinstance(items_in, torch.Tensor):\n",
    "        items_in = torch.Tensor(items_in)\n",
    "    # Shift the input for numerical stability (optional, but recommended)\n",
    "    # items_in = items_in - torch.max(items_in)\n",
    "    # TODO\n",
    "    # Compute the exponential of the input\n",
    "    exp_items = None\n",
    "    # TODO\n",
    "    # Compute the softmax by dividing by the sum of exponentials\n",
    "    items_out = None\n",
    "\n",
    "    return items_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "You will now use your `softmax` implementation to compute the self-attention weights by applying the softmax function to the self-attention scores you computed in <a href=\"#exercise-02\">Exercise 2</a>. Note that the self-attention scores for each input vector: ${\\boldsymbol x_{1}}$, ${\\boldsymbol x_{2}}$, ${\\boldsymbol x_{3}}$, are the contained in the list object `all_attention_scores`.\n",
    "\n",
    "<a id=\"exercise-04\"></a>\n",
    "#### Exercise 4\n",
    "Compute the self-attention weights of the input vectors: ${\\boldsymbol x_{1}}$, ${\\boldsymbol x_{2}}$, and ${\\boldsymbol x_{3}}$ using their self-attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "all_attention_weights = []\n",
    "for idx, attention_scores in enumerate(all_attention_scores):\n",
    "    # TODO\n",
    "    # Compute the attention probabilities using the softmax function\n",
    "    attention_weights = None\n",
    "    print(f\"The attention probabilities for input vector x_{idx+1} are: {attention_weights}\")\n",
    "    all_attention_weights.append(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "```python\n",
    "The attention probabilities for input vector x_1 are: tensor([0.0014, 0.9908, 0.0078])\n",
    "The attention probabilities for input vector x_2 are: tensor([0.0083, 0.5183, 0.4735])\n",
    "The attention probabilities for input vector x_3 are: tensor([3.0824e-01, 3.0549e-04, 6.9145e-01])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<a id=\"self-attention-step-04\"></a>\n",
    "### Self Attention: Step 4\n",
    "\n",
    "The fourth step is to multiply each value vector, i.e., the values contained in the list object **`all_values`**, by their corresponding attention weights, i.e, the values contained in the list object **`all_attention_weights`**. The intuition behind this multiplication is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words, that is, multiplying irrelevant words by tiny numbers like $0.001$, which in this case are the attention weights. \n",
    "\n",
    "After multiplying the attention weights by their corresponding value vectors we sum them up. \n",
    "\n",
    "<a id=\"exercise-05\"></a>\n",
    "#### Exercise 5\n",
    "Use the attention weights you computed in <a href=\"#exercise-04\">Exercise 4</a>, i.e., the contents of **`all_attention_weights`**, and the value vectors, **`all_values`** to compute the weighted sum of all values ${\\boldsymbol v_{1}}, {\\boldsymbol v_{2}}, {\\boldsymbol v_{3}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "all_attention_weighted_values = []\n",
    "\n",
    "# Loop over each set of attention weights\n",
    "for attention_weights in all_attention_weights:\n",
    "    attention_weighted_values = []\n",
    "    \n",
    "    # For each attention weight and corresponding value\n",
    "    for attention_weight, value in zip(attention_weights, all_values):\n",
    "        # TODO\n",
    "        # Compute the weighted value using element-wise multiplication\n",
    "        weighted_value = None\n",
    "        \n",
    "        attention_weighted_values.append(weighted_value)\n",
    "    # TODO\n",
    "    # Sum the weighted values across all values\n",
    "    attention_weight_across_all_values = torch.sum(torch.stack(attention_weighted_values), dim=0)\n",
    "    \n",
    "    # Append the final result to the list\n",
    "    all_attention_weighted_values.append(attention_weight_across_all_values)\n",
    "\n",
    "all_attention_weighted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<a id=\"self-attention-all-steps\"></a>\n",
    "### Self Attention All Steps\n",
    "The figure below \n",
    "\n",
    "![Complete Attention Steps](../images/self-attention-output.png)\n",
    "\n",
    "demonstrate the complete self-attention mechanism. Now let's put everything together in <a href=\"#exercise-06\"> Exercise 6</a> below\n",
    "\n",
    "<a id=\"exercise-06\"></a>\n",
    "#### Exercise 6\n",
    "\n",
    "Implement the complete self attention mechanism by completing the incomplete code snippet below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Create empty list for output\n",
    "all_x_prime = []\n",
    "\n",
    "# Assuming N is defined\n",
    "for n in range(N):\n",
    "    # Create list for dot products of query N with all keys\n",
    "    all_km_qn = []\n",
    "\n",
    "    # Compute the dot products\n",
    "    for key in all_keys:\n",
    "        \n",
    "        # TODO\n",
    "        # Compute the dot product of the key and query\n",
    "        dot_product = torch.matmul(key.T, all_queries[n]).squeeze() \n",
    "        # TODO\n",
    "        # Compute the scaled dot product (Normalizing the dot product by the square root of D)\n",
    "        d_k = None\n",
    "        scaled_dot_product = None\n",
    "        \n",
    "\n",
    "        # Store dot product\n",
    "        all_km_qn.append(scaled_dot_product)\n",
    "\n",
    "    # Convert dot products to a tensor\n",
    "    all_km_qn = torch.tensor(all_km_qn)\n",
    "\n",
    "    # Compute softmax over dot products to get attention\n",
    "    attention = softmax(all_km_qn)\n",
    "\n",
    "    # Print result (should be positive and sum to one)\n",
    "    print(\"Attentions for output \", n)\n",
    "    print(attention)\n",
    "\n",
    "    \n",
    "    # TODO\n",
    "    # Compute the weighted sum of the values using the attention weights\n",
    "    x_prime = None\n",
    "\n",
    "    # Append the result to the output list\n",
    "    all_x_prime.append(x_prime)\n",
    "\n",
    "# Print out true values to check you have it correct\n",
    "print(\"\\nx_prime_0_calculated:\", all_x_prime[0].T)\n",
    "print(\"x_prime_0_true:       tensor([[ 0.2117,  1.0697, -3.3355, -4.9260]])\\n\")\n",
    "print(\"x_prime_1_calculated:\", all_x_prime[1].T)\n",
    "print(\"x_prime_1_true:       tensor([[ 0.6486,  0.9883, -2.4109, -3.0185]])\\n\")\n",
    "print(\"x_prime_2_calculated:\", all_x_prime[2].T)\n",
    "print(\"x_prime_2_true:       tensor([[ 0.6463,  0.8405, -1.6421, -0.0805]])\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As you may have observed, all the computations we have done, could be done using matrices, as illustrated in Figures  \n",
    "\n",
    "![Self Attention Matrix Calculation Step 1](../images/self-attention-matrix-calculation.png)\n",
    "\n",
    "This completes the first step, the remain steps are illustrated in the figure below: \n",
    "\n",
    "![Self Attention Matrix Calculation Step 2](../images/self-attention-matrix-calculation-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "1. Compute the attention scores by multiplying the set of queries packed in matrix $Q$ with the keys in the matrix $K$. If the matrix $Q$ is of size $m \\times d_k$, and the matrix $K$ is of size $n \\times d_k$, then the resulting matrix will be of size $m \\times n$:\n",
    "\n",
    "$$\n",
    "QK^\\top = \\begin{bmatrix}\n",
    "e_{11} & e_{12} & \\dots & e_{1n} \\\\\n",
    "e_{21} & e_{22} & \\dots & e_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "e_{m1} & e_{m2} & \\dots & e_{mn}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "2. Scale each of the alignment scores by $\\frac{1}{\\sqrt{d_k}}$:\n",
    "\n",
    "$$\n",
    "\\frac{QK^\\top}{\\sqrt{d_k}} = \\begin{bmatrix}\n",
    "\\frac{e_{11}}{\\sqrt{d_k}} & \\frac{e_{12}}{\\sqrt{d_k}} & \\dots & \\frac{e_{1n}}{\\sqrt{d_k}} \\\\\n",
    "\\frac{e_{21}}{\\sqrt{d_k}} & \\frac{e_{22}}{\\sqrt{d_k}} & \\dots & \\frac{e_{2n}}{\\sqrt{d_k}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{e_{m1}}{\\sqrt{d_k}} & \\frac{e_{m2}}{\\sqrt{d_k}} & \\dots & \\frac{e_{mn}}{\\sqrt{d_k}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3. And follow the scaling process by applying a softmax operation in order to obtain a set of attention weights:\n",
    "\n",
    "$$\n",
    "\\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) = \\begin{bmatrix}\n",
    "\\text{softmax}\\left( \\frac{e_{11}}{\\sqrt{d_k}} \\right) & \\text{softmax}\\left( \\frac{e_{12}}{\\sqrt{d_k}} \\right) & \\dots & \\text{softmax}\\left( \\frac{e_{1n}}{\\sqrt{d_k}} \\right) \\\\\n",
    "\\text{softmax}\\left( \\frac{e_{21}}{\\sqrt{d_k}} \\right) & \\text{softmax}\\left( \\frac{e_{22}}{\\sqrt{d_k}} \\right) & \\dots & \\text{softmax}\\left( \\frac{e_{2n}}{\\sqrt{d_k}} \\right) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{softmax}\\left( \\frac{e_{m1}}{\\sqrt{d_k}} \\right) & \\text{softmax}\\left( \\frac{e_{m2}}{\\sqrt{d_k}} \\right) & \\dots & \\text{softmax}\\left( \\frac{e_{mn}}{\\sqrt{d_k}} \\right)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "4. Finally, apply the resulting attention weights to the values in matrix $V$, of size $n \\times d_v$:\n",
    "\n",
    "$$\n",
    "\\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) \\cdot V = \n",
    "\\begin{bmatrix}\n",
    "\\text{softmax}\\left( \\frac{e_{11}}{\\sqrt{d_k}} \\right) & \\text{softmax}\\left( \\frac{e_{12}}{\\sqrt{d_k}} \\right) & \\dots & \\text{softmax}\\left( \\frac{e_{1n}}{\\sqrt{d_k}} \\right) \\\\\n",
    "\\text{softmax}\\left( \\frac{e_{21}}{\\sqrt{d_k}} \\right) & \\text{softmax}\\left( \\frac{e_{22}}{\\sqrt{d_k}} \\right) & \\dots & \\text{softmax}\\left( \\frac{e_{2n}}{\\sqrt{d_k}} \\right) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{softmax}\\left( \\frac{e_{m1}}{\\sqrt{d_k}} \\right) & \\text{softmax}\\left( \\frac{e_{m2}}{\\sqrt{d_k}} \\right) & \\dots & \\text{softmax}\\left( \\frac{e_{mn}}{\\sqrt{d_k}} \\right)\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "v_{11} & v_{12} & \\dots & v_{1d_v} \\\\\n",
    "v_{21} & v_{22} & \\dots & v_{2d_v} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{n1} & v_{n2} & \\dots & v_{nd_v}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In <a href=\"#exercise-07\">Exercise 7</a> below, you will use matrices to implement the self-attention mechanism, given the input matrix ${\\boldsymbol X}$. We provide the helper function `softmax_cols` that will be necessary as well as the input matrix ${\\boldsymbol X}$, whose columns are the vectors in **`all_x`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Copy data into a matrix\n",
    "X = torch.zeros((D, N))  # Create a tensor of shape (D, N) filled with zeros\n",
    "\n",
    "# Copy the data from all_x into the matrix\n",
    "X[:, 0] = all_x[0].squeeze()\n",
    "X[:, 1] = all_x[1].squeeze()\n",
    "X[:, 2] = all_x[2].squeeze()\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Define softmax operation that works independently on each column\n",
    "def softmax_cols(data_in):\n",
    "    # Exponentiate all of the values\n",
    "    exp_values = torch.exp(data_in)\n",
    "    \n",
    "    # Sum over columns (dim=0 for column-wise summation)\n",
    "    denom = torch.sum(exp_values, dim=0)\n",
    "    \n",
    "    # Replicate denominator to match the input size\n",
    "    denom = denom.unsqueeze(0).expand_as(data_in)\n",
    "    \n",
    "    # Compute softmax\n",
    "    softmax = exp_values / denom\n",
    "    \n",
    "    # Return the result\n",
    "    return softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<a id=\"exercise-07\"></a>\n",
    "#### Exercise 7\n",
    "Compute the self-attention mechanism in matrix form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Define the scaled dot product self-attention function\n",
    "def scaled_dot_product_self_attention(X, W_v, W_q, W_k, b_v, b_q, b_k):\n",
    "    \n",
    "    # 1. Compute queries, keys, and values using matrix multiplication and addition\n",
    "    # TODO\n",
    "    queries = None\n",
    "    keys = None\n",
    "    values = None\n",
    "\n",
    "    # 2. Compute dot products of keys and queries\n",
    "    # TODO\n",
    "    attention_scores = None\n",
    "\n",
    "    # 3. Scale the dot products by the square root of the dimensionality of the keys\n",
    "    d_k = torch.tensor(keys.shape[0], dtype=torch.float32)  # dimensionality of the keys\n",
    "    # TODO\n",
    "    scaled_attention_scores = None / torch.sqrt(d_k)\n",
    "\n",
    "    # 4. Apply softmax to calculate attention weights (column-wise softmax)\n",
    "    # TODO\n",
    "    attention_weights = None\n",
    "\n",
    "    # 5. Weight the values by attention weights\n",
    "    # TODO\n",
    "    X_prime = None\n",
    "\n",
    "    return X_prime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_prime = scaled_dot_product_self_attention(X, W_v, W_q, W_k, b_v, b_q, b_k)\n",
    "X_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "```python\n",
    "tensor([[ 0.2117,  0.6486,  0.6463],\n",
    "        [ 1.0697,  0.9883,  0.8405],\n",
    "        [-3.3355, -2.4109, -1.6421],\n",
    "        [-4.9260, -3.0185, -0.0805]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<a id=\"multihead-self-attention\"></a>\n",
    "## Multihead Self-Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The multihead self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n} \\in \\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$. In other words, it the repetition of the self-attention a fix number of times in parallel. Self-attention occurs in parallel across multiple \"heads\". Each head has its own queries, keys, and values. The Figure below gives an illustration of a 2-head self attention, in the cyan and orange boxes, respectively. The outputs are vertically concatenated an another linear transformation layer, ${\\boldsymbol \\Omega_{c}}$ is used to recombine them.\n",
    "\n",
    "![Two Head Self Attention](../images/two-head-self-attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Set seed so we get the same random numbers\n",
    "torch.manual_seed(3)\n",
    "\n",
    "# Number of inputs\n",
    "N = 6\n",
    "\n",
    "# Number of dimensions of each input\n",
    "D = 8\n",
    "\n",
    "# Create a tensor with random normal values (mean=0, std=1)\n",
    "mat_X = torch.randn(D, N)\n",
    "\n",
    "# Print X\n",
    "print(mat_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As shown in the Figure we will use $2$ heads. We need the weights matrices and biases vectors for the keys, queries, and values. We'll make the queries keys and values of dimensions $\\frac{D}{H} \\times N$, as shown in the Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "H = 2\n",
    "# QKV dimension\n",
    "H_D = int(D / H)\n",
    "\n",
    "# Set seed so we get the same random numbers\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Choose random values for the parameters for the first head\n",
    "W_q1 = torch.randn(size=(H_D, D))\n",
    "W_k1 = torch.randn(size=(H_D, D))\n",
    "W_v1 = torch.randn(size=(H_D, D))\n",
    "b_q1 = torch.randn(size=(H_D, 1))\n",
    "b_k1 = torch.randn(size=(H_D, 1))\n",
    "b_v1 = torch.randn(size=(H_D, 1))\n",
    "\n",
    "# Choose random values for the parameters for the second head\n",
    "W_q2 = torch.randn(size=(H_D, D))\n",
    "W_k2 = torch.randn(size=(H_D, D))\n",
    "W_v2 = torch.randn(size=(H_D, D))\n",
    "b_q2 = torch.randn(size=(H_D, 1))\n",
    "b_k2 = torch.randn(size=(H_D, 1))\n",
    "b_v2 = torch.randn(size=(H_D, 1))\n",
    "\n",
    "# Choose random values for the parameters\n",
    "W_c = torch.randn(size=(D, D)) # Linear transformation used to combine the vertically concatenated attention heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As before, a helper fuction `multi_head_softmax_cols` is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Define softmax operation that works independently on each column\n",
    "def multi_head_softmax_cols(data_in):\n",
    "    # Exponentiate all of the values\n",
    "    exp_values = torch.exp(data_in)\n",
    "    \n",
    "    # Sum over columns (dim=0 for column-wise summation)\n",
    "    denom = torch.sum(exp_values, dim=0, keepdim=True)\n",
    "    \n",
    "    # Compute softmax (PyTorch broadcasts the denominator to all rows automatically)\n",
    "    softmax = exp_values / denom\n",
    "    \n",
    "    # Return the result\n",
    "    return softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<a id=\"exercise-08\"></a>\n",
    "#### Exercise 8\n",
    "\n",
    "Implement the multihead self-attention mechanism by completing the code snippets below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Define the multi-head scaled self-attention mechanism\n",
    "def multihead_scaled_self_attention(\n",
    "        X, W_v1, W_q1, W_k1, b_v1, b_q1, b_k1, W_v2, W_q2, W_k2, b_v2, b_q2, b_k2, W_c\n",
    "    ):\n",
    "    \n",
    "    # 1. Compute queries, key, and value for Head 1\n",
    "    Q1 = torch.matmul(W_q1, X) + b_q1\n",
    "    # TODO\n",
    "    K1 = None\n",
    "    V1 = None\n",
    "\n",
    "    # 2. Compute queries, key, and value for Head 2\n",
    "    # TODO\n",
    "    Q2 = None\n",
    "    K2 = None\n",
    "    V2 = None\n",
    "\n",
    "    # 3. Compute dot products\n",
    "    # TODO\n",
    "    dot_products1 = None\n",
    "    dot_products2 = None\n",
    "\n",
    "    d_k = torch.tensor(K1.shape[0], dtype=torch.float32)  # dimensionality of the keys (same for both heads)\n",
    "\n",
    "    # 4. Scale dot products\n",
    "    # TODO\n",
    "    scaled_dot_products1 = None\n",
    "    scaled_dot_products2 = None\n",
    "\n",
    "    # 5. Apply softmax to calculate attention scores\n",
    "    # TODO\n",
    "    attentions1 = None\n",
    "    attentions2 = None\n",
    "\n",
    "    # 6. Weight values by attention weights\n",
    "    # TODO\n",
    "    head1_output = None\n",
    "    head2_output = None\n",
    "\n",
    "    # 7. Concatenate the outputs of the two heads\n",
    "    # TODO\n",
    "    concatenated_output = None\n",
    "\n",
    "    # 8. Apply the final linear transformation\n",
    "    # TODO\n",
    "    X_prime = None\n",
    "\n",
    "    return X_prime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_prime = multihead_scaled_self_attention(\n",
    "    X=mat_X,\n",
    "    W_v1=W_v1,\n",
    "    W_q1=W_q1, \n",
    "    W_k1=W_k1,\n",
    "    b_v1=b_v1,\n",
    "    b_q1=b_q1,\n",
    "    b_k1=b_k1,\n",
    "    W_v2=W_v2,\n",
    "    W_q2=W_q2,\n",
    "    W_k2=W_k2,\n",
    "    b_v2=b_v2,\n",
    "    b_q2=b_q2,\n",
    "    b_k2=b_k2,\n",
    "    W_c=W_c\n",
    ")\n",
    "\n",
    "# Set precision for printing\n",
    "# torch.set_printoptions(precision=3)\n",
    "\n",
    "# Print out the results\n",
    "print(\"Your answer:\")\n",
    "print(X_prime)\n",
    "\n",
    "print(\"\\nTrue values:\")\n",
    "true_values = torch.tensor([[  7.501,  15.386,  12.121,  23.458,   5.546,  -7.499],\n",
    "                            [  4.221,   4.875,  -2.205,   4.050,  -4.525,   5.155],\n",
    "                            [  1.891,   3.035,   3.399,   2.733,   2.958,  -0.824],\n",
    "                            [  2.621,   2.177,  -4.974,  -0.925,  -1.928,   3.726],\n",
    "                            [ -0.130,  -0.250,   3.700,   0.948,   9.384,   0.697],\n",
    "                            [  2.524,   1.555,  -0.789,   2.667,  -0.459,   4.428],\n",
    "                            [  0.056,  -1.688,  -1.537,  -1.700,   0.391,   4.648],\n",
    "                            [ -1.352,  -4.136,  -8.878,  -1.003, -12.857,  -4.945]])\n",
    "\n",
    "print(true_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "# model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "# model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "\n",
    "device = \"cpu\" # the device to load the model onto\n",
    "device_map = \"cpu\"  # \"auto\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "# The transformers is a Python library that provides general-purpose architectures \n",
    "# for natural language understanding and natural language generation. \n",
    "# It is based on the PyTorch library.\n",
    "# Each model is associated with a tokenizer that is used to preprocess the input text.\n",
    "# Load the model and tokenizer framework\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# Load the Qwen2.5-0.5B-Instruct pretrained model \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A large language model is a type of artificial intelligence that can generate human-like text using natural language processing techniques. These models have been trained on vast amounts of data, including books, articles, and internet forums, to learn how to understand and produce coherent sentences. Large language models are often used in applications such as chatbots, virtual assistants, and machine translation systems.'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input text\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "\n",
    "\n",
    "max_new_tokens=100\n",
    "\n",
    "\n",
    "# Apply the chat template to the input text\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# \n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    attention_mask=model_inputs.attention_mask,\n",
    "    max_new_tokens=max_new_tokens\n",
    ")\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(sequences=generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
       "         151645,    198, 151644,    872,    198,  35127,    752,    264,   2805,\n",
       "          16800,    311,   3460,   4128,   1614,     13, 151645,    198, 151644,\n",
       "          77091,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "rise": {
   "autolaunch": false,
   "enable_chalkboard": true,
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
