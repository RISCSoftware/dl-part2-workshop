{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"transformers\"></a>\n",
    "## Transformers - High Level Motivation:\n",
    "How does the **ChatGPT** model predicts next word, given an input word?\n",
    "    <p align=\"center\">\n",
    "        <div style=\"display: flex; justify-content: space-around;\">\n",
    "            <img src=\"../images/next-token-prediction.png\" alt=\"Image 1\" style=\"width:40%; margin-top:25px; margin-right:10px; \">\n",
    "            <img src=\"../images/probability-distribution.png\" alt=\"Image 2\" style=\"width:40%; margin-top:25px;\">\n",
    "        </div>\n",
    "    </p>\n",
    "\n",
    "**ChatGPT** is based on the transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformers - A High Level Motivation\n",
    " \n",
    "- The transformer architecture was introduced in the [scientific paper](https://arxiv.org/pdf/1706.03762)\n",
    "  <p align=\"center\">\n",
    "    <div style=\"display: flex; justify-content: space-around;\">\n",
    "      <img src=\"../images/attention-is-all-you-need.png\" alt=\"Attention Is All You Need\", style=\"width:35%; margin-top:10px;\">\n",
    "      <img src=\"../images/transformer-architecture.png\" alt=\"Attention Is All You Need\", style=\"width:60%; margin-top:10px;\">\n",
    "  </div>\n",
    "  </p>\n",
    "- It uses a key mechanism called **self-attention**, which lets each word in a sequence focus on all others to capture their relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Preprocessing \n",
    "<div style=\"display: flex; align-items: left;\">\n",
    "    <div style=\"flex: 1;\">\n",
    "        <ul>\n",
    "            <li>Input text is tokenized.</li>\n",
    "            <li>Tokens are converted to embeddings.</li>\n",
    "            <li>Positional encodings are added.</li>\n",
    "            <li>Resulting vectors are fed into the self-attention layer.</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    <div style=\"flex: 1;\">\n",
    "        <p align=\"left\">\n",
    "          <img src=\"../images/transformer_positional_encoding_vectors.png\" alt=\"Preprocessing\" style=\"width:70%; margin-left: 10px;\">\n",
    "        </p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<p style=\"text-align: left; margin-top: 20px;\">\n",
    "    <ul>\n",
    "        <li>We will now concentrate on the <b>self-attention layer</b> and implement it.</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"self-attention\"></a>\n",
    "## Self-Attention\n",
    "\n",
    "The _self-attention_ block in the transformer architecture takes $N$ inputs: ${\\boldsymbol x_{1}},\\, {\\boldsymbol x_{2}}, \\dots, {\\boldsymbol x_{N}}$, each of size $D \\times 1$, and returns $N$ output vectors. The process is as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"self-attention-step-01\"></a>\n",
    "### Self-Attention: Step 1\n",
    "For each input word ${\\boldsymbol x}$, generate:\n",
    "- A _query_ vector ${\\boldsymbol q}$, a _key_ vector ${\\boldsymbol k}$, and a _value_ vector ${\\boldsymbol v}$.\n",
    "- To compute these, use:\n",
    "  - Weight matrix ${\\boldsymbol W_{q}}$ and bias vector ${\\boldsymbol b_{q}}$ for queries.\n",
    "  - Weight matrix ${\\boldsymbol W_{k}}$ and bias vector ${\\boldsymbol b_{k}}$ for keys.\n",
    "  - Weight matrix ${\\boldsymbol W_{v}}$ and bias vector ${\\boldsymbol b_{v}}$ for values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Implement the computation of _queries_, _keys_, and _values_ using the [torch](https://pytorch.org/) Python Library.\n",
    "- Generate input vectors ${\\boldsymbol x_{1}}$, ${\\boldsymbol x_{2}}$, and ${\\boldsymbol x_{3}}$ with dimensions $D \\times 1$, with $D=4$.\n",
    "- Define weight matrices and bias vectors: \n",
    "  - ${\\boldsymbol W_{q}}, {\\boldsymbol b_{q}}$ for queries\n",
    "  - ${\\boldsymbol W_{k}}, {\\boldsymbol b_{k}}$ for keys\n",
    "  - ${\\boldsymbol W_{v}}, {\\boldsymbol b_{v}}$ for values\n",
    "- Compute the _queries_, _keys_, and _values_ using these weights and biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set seed so we get the same random numbers\n",
    "torch.manual_seed(3)\n",
    "\n",
    "# Number of inputs\n",
    "N = 3\n",
    "\n",
    "# Number of dimensions of each input\n",
    "D = 4\n",
    "\n",
    "# Create an empty list\n",
    "all_x = []\n",
    "# Create elements x_n and append to list\n",
    "for n in range(N):\n",
    "    # Creates a tensor of shape (D, 1) with values drawn from the standard normal distribution, \n",
    "    # with a mean of 0 and a standard deviation of 1 \n",
    "    x = torch.randn(size=(D, 1))  # D x 1 tensor\n",
    "    print(f\"The vector x_{n+1} is: \\n {x} \\n\")\n",
    "    all_x.append(x)  # Append x to list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "After executing the cell above, the list object, `all_x` now has $N$ vectors, each of dimension $D \\times 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's define the weight matrices and bias vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Set seed so we get the same random numbers\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Choose random values for the parameters\n",
    "\n",
    "# weight matrices\n",
    "W_q = torch.randn(size=(D, D))\n",
    "W_k = torch.randn(size=(D, D))\n",
    "W_v = torch.randn(size=(D, D))\n",
    "\n",
    "# bais terms\n",
    "b_q = torch.randn(size=(D, 1))\n",
    "b_k = torch.randn(size=(D, 1))\n",
    "b_v = torch.randn(size=(D, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"exercise-01\"></a>\n",
    "#### Exercise 1\n",
    "Given vectors ${\\boldsymbol x}_{1}$, ${\\boldsymbol x}_{2}$, and ${\\boldsymbol x}_{3}$ in **`all_x`**, and weight matrices ${\\boldsymbol W_{q}}$, ${\\boldsymbol W_{k}}$, ${\\boldsymbol W_{v}}$, and bias vectors ${\\boldsymbol b_{q}}$, ${\\boldsymbol b_{k}}$, ${\\boldsymbol b_{v}}$, compute:\n",
    "  - Query vectors: ${\\boldsymbol q_{1}}$, ${\\boldsymbol q_{2}}$, ${\\boldsymbol q_{3}}$\n",
    "  - Key vectors: ${\\boldsymbol k_{1}}$, ${\\boldsymbol k_{2}}$, ${\\boldsymbol k_{3}}$\n",
    "  - Value vectors: ${\\boldsymbol v_{1}}$, ${\\boldsymbol v_{2}}$, ${\\boldsymbol v_{3}}$\n",
    "\n",
    "- Formula: \n",
    "  - ${\\boldsymbol q} = {\\boldsymbol b} + {\\boldsymbol W}\\, {\\boldsymbol x}$\n",
    "  - ${\\boldsymbol k} = {\\boldsymbol b} + {\\boldsymbol W}\\, {\\boldsymbol x}$\n",
    "  - ${\\boldsymbol v} = {\\boldsymbol b} + {\\boldsymbol W}\\, {\\boldsymbol x}$\n",
    "\n",
    "- Use [torch.matmul](https://pytorch.org/docs/stable/generated/torch.matmul.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"#exercise-1\">Exercise 1</a> code snippet for you to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Make three lists to store queries, keys, and values\n",
    "all_queries = []\n",
    "all_keys = []\n",
    "all_values = []\n",
    "\n",
    "# For every input\n",
    "for x in all_x:\n",
    "    # TODO\n",
    "    # Compute query, key, and value\n",
    "    query = None \n",
    "    key = None\n",
    "    value = None\n",
    "\n",
    "    # Append the results to the lists\n",
    "    all_queries.append(query)\n",
    "    all_keys.append(key)\n",
    "    all_values.append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A visual illustration of <a href=\"#self-attention-step-1\"> Self Attention: Step 1</a>\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../images/self_attention_step_01.png\" alt=\"self attention step 1\", style=\"width:50%; margin-top:25px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Self Attention: Step 2\n",
    "\n",
    "- **Objective**: Calculate a score for self-attention.\n",
    "- **Example**: For the word _\"Thinking\"_ in the sentence _\"Thinking Machine\"_.\n",
    "- **Purpose**: Score each word against _\"Thinking\"_ to determine how much focus to place on other parts of the sentence during encoding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Self Attention: Step 2\n",
    "\n",
    "- **Attention Score Calculation**:\n",
    "  - Use the **dot product** of the _query_ vector of the word being processed with the _key_ vectors of all words.\n",
    "  - For word 1 (_\"Thinking\"_):\n",
    "    - First score: **dot product** of ${\\boldsymbol q_{1}}$ and ${\\boldsymbol k_{1}}$.\n",
    "    - Second score: **dot product** of ${\\boldsymbol q_{1}}$ and ${\\boldsymbol k_{2}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Self Attention: Step 2\n",
    "  \n",
    "- **Visual llustration**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../images/self_attention_step_02.png\" alt=\"self attention step 2\", style=\"width:50%; margin-top:25px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"exercise-02\"> </a>\n",
    "#### Exercise 2\n",
    "\n",
    "Compute the _self-attention scores_ of the input vectors ${\\boldsymbol x_{1}}$, ${\\boldsymbol x_{2}}$, ${\\boldsymbol x_{3}}$, using their query vectors ${\\boldsymbol q_{1}}$, ${\\boldsymbol q_{2}}$, ${\\boldsymbol q_{3}}$ and key vectors: ${\\boldsymbol k_{1}}$, ${\\boldsymbol k_{2}}$, ${\\boldsymbol k_{3}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "all_attention_scores = []\n",
    "for query in all_queries:\n",
    "    query_keys_attention_scores = []\n",
    "    for key in all_keys:\n",
    "        # Compute the dot product of the query and key\n",
    "        # TODO\n",
    "        # Compute the dot product\n",
    "        dot_product = None\n",
    "        # TODO\n",
    "        # Compute the scaled dot product (Normalizing the dot product by the square root of D)\n",
    "        # Use the math.sqrt() function to get the square root of D\n",
    "        # The purpose of normalization is for numerical stability\n",
    "        scaled_dot_product = None\n",
    "\n",
    "        # Append the result to the list\n",
    "        query_keys_attention_scores.append(scaled_dot_product)\n",
    "    all_attention_scores.append(query_keys_attention_scores)\n",
    "    \n",
    "all_attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Your output should be\n",
    "```python\n",
    "[[tensor(-2.8869), tensor(3.6531), tensor(-1.1920)],\n",
    " [tensor(0.0760), tensor(4.2115), tensor(4.1211)],\n",
    " [tensor(0.8152), tensor(-6.1015), tensor(1.6231)]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"self-attention-step-03\"></a>\n",
    "### Self Attention: Step 3\n",
    "\n",
    "- Normalize each attention score so that they are positive and sum to $1$. \n",
    "- This is done using the [softmax function](https://en.wikipedia.org/wiki/Softmax_function), $\\sigma: \\mathbb{R}^{m} \\to (0, 1)^{m}$, defined as:\n",
    "\n",
    "Given the vector ${\\boldsymbol z} = (z_{1},\\, z_{2},\\,\\dots,\\, z_{m}) \\in \\mathbb{R}^{m}$, \n",
    "\n",
    "$$\\sigma({\\boldsymbol z})_{i} = \\dfrac{e^{z_{i}}}{\\sum_{j=1}^{m} e^{z_{j}}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"exercise-03\"></a>\n",
    "#### Exercise 3\n",
    "Implement the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(items_in):\n",
    "    if not isinstance(items_in, torch.Tensor):\n",
    "        items_in = torch.Tensor(items_in)\n",
    "    # TODO\n",
    "    # Compute the exponential of the input\n",
    "    exp_items = None\n",
    "    # TODO\n",
    "    # Compute the softmax by dividing by the sum of exponentials\n",
    "    items_out = None\n",
    "\n",
    "    return items_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Use your `softmax` implementation to compute the _self-attention weights_ by applying it to the _self-attention scores_ calculated in <a href=\"#exercise-02\">Exercise 2</a>. \n",
    "\n",
    "- The _self-attention scores_ for each input vector: ${\\boldsymbol x_{1}}$, ${\\boldsymbol x_{2}}$, and ${\\boldsymbol x_{3}}$ are stored in the list object `all_attention_scores`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<a id=\"exercise-04\"></a>\n",
    "#### Exercise 4\n",
    "Compute the self-attention weights of the input vectors: ${\\boldsymbol x_{1}}$, ${\\boldsymbol x_{2}}$, and ${\\boldsymbol x_{3}}$ using their self-attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "all_attention_weights = []\n",
    "for idx, attention_scores in enumerate(all_attention_scores):\n",
    "    # TODO\n",
    "    # Compute the self-attention weights using the softmax function\n",
    "    attention_weights = None \n",
    "    print(f\"The attention weights for input vector x_{idx+1} are: {attention_weights}\")\n",
    "    all_attention_weights.append(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Your output should be:\n",
    "```python\n",
    "The attention weights for input vector x_1 are: tensor([0.0014, 0.9908, 0.0078])\n",
    "The attention weights for input vector x_2 are: tensor([0.0083, 0.5183, 0.4735])\n",
    "The attention weights for input vector x_3 are: tensor([3.0824e-01, 3.0549e-04, 6.9145e-01])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"self-attention-step-04\"></a>\n",
    "### Self Attention: Step 4\n",
    "\n",
    "- Multiply each value vector in **`all_values`** by its corresponding attention weight from **`all_attention_weights`**.\n",
    "- This retains important values while reducing the influence of less relevant words using small attention weights.\n",
    "- After multiplication, sum the weighted value vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<a id=\"exercise-05\"></a>\n",
    "#### Exercise 5\n",
    "Use the attention weights from <a href=\"#exercise-04\">Exercise 4</a> (stored in **`all_attention_weights`**) and the value vectors in **`all_values`** to compute the weighted sum of ${\\boldsymbol v_{1}}, {\\boldsymbol v_{2}}, {\\boldsymbol v_{3}}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "all_attention_weighted_values = []\n",
    "\n",
    "# Loop over each set of attention weights\n",
    "for attention_weights in all_attention_weights:\n",
    "    attention_weighted_values = []\n",
    "    \n",
    "    # For each attention weight and corresponding value\n",
    "    for attention_weight, value in zip(attention_weights, all_values):\n",
    "        # TODO\n",
    "        # Compute the scalar vector multiplication\n",
    "        weighted_value = None\n",
    "        \n",
    "        attention_weighted_values.append(weighted_value)\n",
    "    # TODO\n",
    "    # Sum the weighted values across all values\n",
    "    # Use torch.stack() to stack the weighted values\n",
    "    # Use torch.sum() to sum the values\n",
    "    attention_weight_across_all_values = None\n",
    "    \n",
    "    # Append the final result to the list\n",
    "    all_attention_weighted_values.append(attention_weight_across_all_values)\n",
    "\n",
    "all_attention_weighted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Your output should be: \n",
    "\n",
    "```python\n",
    "[tensor([[ 0.2117],\n",
    "         [ 1.0697],\n",
    "         [-3.3355],\n",
    "         [-4.9260]]),\n",
    " tensor([[ 0.6486],\n",
    "         [ 0.9883],\n",
    "         [-2.4109],\n",
    "         [-3.0185]]),\n",
    " tensor([[ 0.6463],\n",
    "         [ 0.8405],\n",
    "         [-1.6421],\n",
    "         [-0.0805]])]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"self-attention-all-steps\"></a>\n",
    "### Self Attention: All Steps\n",
    "A visual illustration of all **self-attention** mechanism steps.\n",
    "<p align=\"center\">\n",
    "  <img src=\"../images/self-attention-output.png\" alt=\"Complete Attention Steps\", style=\"width:50%; margin-top:25px;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"self-attention-matrix-computations\"></a>\n",
    "### Self-Attention: Matrix Computation\n",
    "All computations could be done using matrices. Step 1 is as follows \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../images/self-attention-matrix-calculation.png\" alt=\"Self Attention Matrix Calculation Step 1\", style=\"width:40%; margin-top:20px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Self-Attention: Matrix Computation\n",
    "The remain steps are illustrated in the figure below:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../images/self-attention-matrix-calculation-2.png\" alt=\"Self Attention Matrix Calculation Step 2\", style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Self-Attention: Matrix Computation: Step 1\n",
    "\n",
    "Compute the **attention scores** by multiplying the set of queries packed in matrix $Q$ with the keys in the matrix $K$. If the matrix $Q$ is of size $m \\times d_k$, and the matrix $K$ is of size $n \\times d_k$, then the resulting matrix will be of size $m \\times n$:\n",
    "\n",
    "$$\n",
    "QK^\\top = \\begin{bmatrix}\n",
    "e_{11} & e_{12} & \\dots & e_{1n} \\\\\n",
    "e_{21} & e_{22} & \\dots & e_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "e_{m1} & e_{m2} & \\dots & e_{mn}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Self-Attention: Matrix Computation: Step 2\n",
    "\n",
    "Scale each of the **attention scores** by $\\frac{1}{\\sqrt{d_k}}$:\n",
    "\n",
    "$$\n",
    "\\frac{QK^\\top}{\\sqrt{d_k}} = \\begin{bmatrix}\n",
    "\\frac{e_{11}}{\\sqrt{d_k}} & \\frac{e_{12}}{\\sqrt{d_k}} & \\dots & \\frac{e_{1n}}{\\sqrt{d_k}} \\\\\n",
    "\\frac{e_{21}}{\\sqrt{d_k}} & \\frac{e_{22}}{\\sqrt{d_k}} & \\dots & \\frac{e_{2n}}{\\sqrt{d_k}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{e_{m1}}{\\sqrt{d_k}} & \\frac{e_{m2}}{\\sqrt{d_k}} & \\dots & \\frac{e_{mn}}{\\sqrt{d_k}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Self-Attention: Matrix Computation: Step 3\n",
    "\n",
    "Apply the softmax function, $\\sigma: \\mathbb{R}^{m} \\to (0, 1)^{m}$, to obtain a set of **attention weights**:\n",
    "\n",
    "$$\n",
    "\\sigma\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) = \\begin{bmatrix}\n",
    "\\sigma\\left( \\frac{e_{11}}{\\sqrt{d_k}} \\right)_{11} & \\sigma\\left( \\frac{e_{12}}{\\sqrt{d_k}} \\right)_{12} & \\dots & \\sigma\\left( \\frac{e_{1n}}{\\sqrt{d_k}} \\right)_{1n} \\\\\n",
    "\\sigma\\left( \\frac{e_{21}}{\\sqrt{d_k}} \\right)_{21} & \\sigma\\left( \\frac{e_{22}}{\\sqrt{d_k}} \\right)_{22} & \\dots & \\sigma\\left( \\frac{e_{2n}}{\\sqrt{d_k}} \\right)_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\sigma\\left( \\frac{e_{m1}}{\\sqrt{d_k}} \\right)_{m1} & \\sigma\\left( \\frac{e_{m2}}{\\sqrt{d_k}} \\right)_{m2} & \\dots & \\sigma\\left( \\frac{e_{mn}}{\\sqrt{d_k}} \\right)_{mn}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Self-Attention: Matrix Computation: Step 4\n",
    "\n",
    "Finally, apply the resulting attention weights to the values in matrix $V$, of size $n \\times d_v$:\n",
    "\n",
    "$$\n",
    "\\sigma\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) \\cdot V = \n",
    "\\begin{bmatrix}\n",
    "\\sigma\\left( \\frac{e_{11}}{\\sqrt{d_k}} \\right)_{11} & \\dots & \\sigma\\left( \\frac{e_{1n}}{\\sqrt{d_k}} \\right)_{1n} \\\\\n",
    "\\sigma\\left( \\frac{e_{21}}{\\sqrt{d_k}} \\right)_{21} & \\dots & \\sigma\\left( \\frac{e_{2n}}{\\sqrt{d_k}} \\right)_{2n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\sigma\\left( \\frac{e_{m1}}{\\sqrt{d_k}} \\right)_{m1} & \\dots & \\sigma\\left( \\frac{e_{mn}}{\\sqrt{d_k}} \\right)_{mn}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "v_{11} & \\dots & v_{1d_v} \\\\\n",
    "v_{21} & \\dots & v_{2d_v} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "v_{n1} & \\dots & v_{nd_v}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In [Exercise 6](#exercise-06), you will implement self-attention using matrices.\n",
    "- The input matrix ${\\boldsymbol X}$ consists of the columns from **`all_x`**.\n",
    "- Apply the _softmax_ function using [torch.softmax](https://pytorch.org/docs/stable/generated/torch.softmax.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Copy data into a matrix\n",
    "X = torch.zeros((D, N))  # Create a tensor of shape (D, N) filled with zeros\n",
    "\n",
    "# Copy the data from all_x into the matrix\n",
    "X[:, 0] = all_x[0].squeeze()\n",
    "X[:, 1] = all_x[1].squeeze()\n",
    "X[:, 2] = all_x[2].squeeze()\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"exercise-06\"></a>\n",
    "#### Exercise 6\n",
    "Given the input matrix: ${\\boldsymbol X}$, the weight matrices: ${\\boldsymbol W_{v}}$, ${\\boldsymbol W_{q}}$, ${\\boldsymbol W_{k}}$, and the bias vectors: ${\\boldsymbol b_{v}}$, ${\\boldsymbol b_{q}}$, ${\\boldsymbol b_{k}}$, implement the matrix form of the self-attention mechanism:\n",
    "\n",
    "$$\\sigma\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) \\cdot V$$\n",
    "\n",
    "For matrix transpose, use the `.T` operator.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Define the scaled dot product self-attention function\n",
    "def scaled_dot_product_self_attention(X, W_v, W_q, W_k, b_v, b_q, b_k):\n",
    "    ## Step 1: Compute queries, keys, and values\n",
    "    # 1. Compute queries, keys, and values using matrix multiplication and addition\n",
    "    # TODO\n",
    "    queries = None\n",
    "    keys = None\n",
    "    values = None\n",
    "\n",
    "    # 2. Compute dot products of keys and queries (keys.T * queries)\n",
    "    # TODO\n",
    "    attention_scores = None\n",
    "\n",
    "    ## Step 2: Perform the scaling operation\n",
    "    # 3. Scale the dot products by the square root of the dimensionality of the keys\n",
    "    d_k = torch.tensor(keys.shape[0], dtype=torch.float32) \n",
    "    # TODO\n",
    "    # Use torch.sqrt() to get the square root of d_k\n",
    "    scaled_attention_scores = None\n",
    "\n",
    "    ## Step 3: Perform the softmax operation to calculate attention weights\n",
    "    # 4. Apply softmax to calculate attention weights, i.e., apply softmax \n",
    "    # to the columns of scaled_attention_scores (i.e., across rows, dimension 0) \n",
    "    # TODO\n",
    "    # Note the dimension you want to apply the softmax to. \n",
    "    # You want to apply it across the rows\n",
    "    attention_weights = None\n",
    "\n",
    "    ## Step 4: Calculate the output\n",
    "    # 5. Weight the values by attention weights\n",
    "    # TODO\n",
    "    X_prime = None\n",
    "\n",
    "    return X_prime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_prime = scaled_dot_product_self_attention(X, W_v, W_q, W_k, b_v, b_q, b_k)\n",
    "X_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "```python\n",
    "tensor([[ 0.2117,  0.6486,  0.6463],\n",
    "        [ 1.0697,  0.9883,  0.8405],\n",
    "        [-3.3355, -2.4109, -1.6421],\n",
    "        [-4.9260, -3.0185, -0.0805]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"multihead-self-attention\"></a>\n",
    "## Multihead Self-Attention \n",
    "\n",
    "- Multihead self-attention maps $N$ input vectors ${\\boldsymbol x_{i}} \\in \\mathbb{R}^{D}$ to $N$ output vectors ${\\boldsymbol x^{'}_{i}}\\in \\mathbb{R}^{D}$ with $1 \\leq i \\le N$.\n",
    "- It repeats the self-attention mechanism a fixed number of times in parallel.\n",
    "- Self-attention occurs across multiple \"heads\", each with its own queries, keys, and values.\n",
    "- Outputs are vertically concatenated and recombined using a linear transformation layer, ${\\boldsymbol \\Omega_{c}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multihead Self-Attention\n",
    "Visual illustration of a 2 head self-atttention.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../images/two-head-self-attention.png\" alt=\"Two Head Attention\", style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Input Requirements for Inplementing 2 Head Self Attention Mechanism\n",
    "\n",
    "- In [Exercise 7](#exercise-07), you will implement the a two head self-attention using matrices.\n",
    "- Given the input matrix ${\\boldsymbol X}$ of dimension $N \\times D$, with $N=6$ and $D=8$.\n",
    "- Apply the _softmax_ function using [torch.softmax](https://pytorch.org/docs/stable/generated/torch.softmax.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set seed so we get the same random numbers\n",
    "torch.manual_seed(3)\n",
    "\n",
    "# Number of inputs\n",
    "N = 6\n",
    "\n",
    "# Number of dimensions of each input\n",
    "D = 8\n",
    "\n",
    "# Create a tensor with random normal values (mean=0, std=1)\n",
    "X = torch.randn(D, N)\n",
    "\n",
    "# Print X\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Input Requirements for Inplementing 2 Head Self Attention Mechanism\n",
    "\n",
    "- Weight matrices and bias vectors are needed for keys, queries, and values.\n",
    "- Queries, keys, and values will have dimensions $\\frac{D}{H} \\times N$, with $H=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "H = 2\n",
    "# QKV dimension\n",
    "H_D = int(D / H)\n",
    "\n",
    "# Set seed so we get the same random numbers\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Choose random values for the parameters for the first head\n",
    "W_q1 = torch.randn(size=(H_D, D))\n",
    "W_k1 = torch.randn(size=(H_D, D))\n",
    "W_v1 = torch.randn(size=(H_D, D))\n",
    "b_q1 = torch.randn(size=(H_D, 1))\n",
    "b_k1 = torch.randn(size=(H_D, 1))\n",
    "b_v1 = torch.randn(size=(H_D, 1))\n",
    "\n",
    "# Choose random values for the parameters for the second head\n",
    "W_q2 = torch.randn(size=(H_D, D))\n",
    "W_k2 = torch.randn(size=(H_D, D))\n",
    "W_v2 = torch.randn(size=(H_D, D))\n",
    "b_q2 = torch.randn(size=(H_D, 1))\n",
    "b_k2 = torch.randn(size=(H_D, 1))\n",
    "b_v2 = torch.randn(size=(H_D, 1))\n",
    "\n",
    "# Choose random values for the parameters\n",
    "W_c = torch.randn(size=(D, D)) # Linear transformation used to combine the vertically concatenated attention heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"exercise-07\"></a>\n",
    "#### Exercise 7\n",
    "\n",
    "Implement the multihead self-attention mechanism by completing the code snippets below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the multi-head scaled self-attention mechanism\n",
    "def multihead_scaled_self_attention(\n",
    "        X, W_v1, W_q1, W_k1, b_v1, b_q1, b_k1, W_v2, W_q2, W_k2, b_v2, b_q2, b_k2, W_c\n",
    "    ):\n",
    "    \n",
    "    # 1. Compute queries, key, and value for Head 1\n",
    "    Q_1 = torch.matmul(W_q1, X) + b_q1\n",
    "    # TODO\n",
    "    K_1 = None\n",
    "    V_1 = None\n",
    "\n",
    "    # 2. Compute queries, key, and value for Head 2\n",
    "    # TODO\n",
    "    Q_2 = None\n",
    "    K_2 = None\n",
    "    V_2 = None\n",
    "\n",
    "    # 3. Compute dot products (Remmber to transpose the keys)\n",
    "    # TODO\n",
    "    attention_scores_1 = None\n",
    "    attention_scores_2 = None\n",
    "    \n",
    "    # dimensionality of the keys (same for both heads)\n",
    "    d_k = torch.tensor(K_1.shape[0], dtype=torch.float32)  \n",
    "\n",
    "    # 4. Scale dot products\n",
    "    # TODO\n",
    "    scaled_attention_scores_1 = None\n",
    "    scaled_atteneion_scores_2 =  None\n",
    "\n",
    "    # 5. Apply softmax to calculate attention scores\n",
    "    # TODO\n",
    "    attention_weights_1 = None \n",
    "    attention_weights_2 = None\n",
    "\n",
    "    # 6. Weight values by attention weights\n",
    "    # TODO\n",
    "    head_1_output = None\n",
    "    head_2_output = None\n",
    "\n",
    "    # 7. Concatenate the outputs of the two heads using torch.cat\n",
    "    # Note the dimension you want to concatenate along. \n",
    "    # You want to concatenate along the rows\n",
    "    # TODO\n",
    "    concatenated_output = None\n",
    "\n",
    "    # 8. Apply the final linear transformation\n",
    "    # TODO\n",
    "    X_prime = None\n",
    "\n",
    "    return X_prime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_prime = multihead_scaled_self_attention(\n",
    "    X=X,\n",
    "    W_v1=W_v1, W_q1=W_q1, W_k1=W_k1, b_v1=b_v1, b_q1=b_q1, b_k1=b_k1,\n",
    "    W_v2=W_v2, W_q2=W_q2, W_k2=W_k2, b_v2=b_v2, b_q2=b_q2, b_k2=b_k2,\n",
    "    W_c=W_c\n",
    ")\n",
    "\n",
    "# Set precision for printing\n",
    "# torch.set_printoptions(precision=3)\n",
    "\n",
    "# Print out the results\n",
    "print(\"Your answer:\")\n",
    "print(X_prime)\n",
    "\n",
    "print(\"\\nTrue values:\")\n",
    "true_values = torch.tensor([[  7.5007,  15.3861,  12.1210,  23.4584,   5.5463,  -7.4986],\n",
    "                            [  4.2205,   4.8746,  -2.2050,   4.0501,  -4.5253,   5.1547],\n",
    "                            [  1.8911,   3.0347,   3.3987,   2.7332,   2.9584,  -0.8235],\n",
    "                            [  2.6210,   2.1765,  -4.9742,  -0.9246,  -1.9279,   3.7258],\n",
    "                            [ -0.1304,  -0.2504,   3.7003,   0.9485,   9.3840,   0.6969],\n",
    "                            [  2.5238,   1.5546,  -0.7890,   2.6671,  -0.4587,   4.4277],\n",
    "                            [  0.0560,  -1.6877,  -1.5374,  -1.7000,   0.3914,   4.6478],\n",
    "                            [ -1.3516,  -4.1357,  -8.8783,  -1.0026, -12.8571,  -4.9448]])\n",
    "\n",
    "print(true_values)\n",
    "\n",
    "# Check if the result is correct\n",
    "assert torch.allclose(X_prime, true_values, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text Generation with Pretrained Transformer Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "# model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "# model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "\n",
    "device = \"cpu\" # the device to load the model onto\n",
    "device_map = \"cpu\"  # \"auto\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "# The transformers is a Python library that provides general-purpose architectures \n",
    "# for natural language understanding and natural language generation. \n",
    "# It is based on the PyTorch library.\n",
    "# Each model is associated with a tokenizer that is used to preprocess the input text.\n",
    "# Load the model and tokenizer framework\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the Qwen2.5-0.5B-Instruct pretrained model \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# Load the tokenizer for the Qwen2.5-0.5B-Instruct pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Input text\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "\n",
    "## Maximum number of tokens to generate\n",
    "max_new_tokens=100\n",
    "\n",
    "## Apply the chat template to the input text\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "\n",
    "## Tokenization\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Generate at most max_new_tokens tokens\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    attention_mask=model_inputs.attention_mask,\n",
    "    max_new_tokens=max_new_tokens\n",
    ")\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "\n",
    "## Decode the generated token ids\n",
    "response = tokenizer.batch_decode(sequences=generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "rise": {
   "autolaunch": false,
   "enable_chalkboard": true,
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
