{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import coloredlogs\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "coloredlogs.install(level='INFO')  # Change this to DEBUG to see more info.\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../nma_rl_games/alpha-zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2023 has been set.\n",
      "WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -> `Change runtime type.`  select `GPU` \n"
     ]
    }
   ],
   "source": [
    "from ws3_helper import set_seed\n",
    "from ws3_helper import seed_worker\n",
    "from ws3_helper import set_device\n",
    "\n",
    "SEED = 2023\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Arena\n",
    "\n",
    "from utils import *\n",
    "from Game import Game\n",
    "from MCTS import MCTS\n",
    "from NeuralNet import NeuralNet\n",
    "\n",
    "# from othello.OthelloPlayers import *\n",
    "from othello.OthelloLogic import Board\n",
    "# from othello.OthelloGame import OthelloGame\n",
    "from othello.pytorch.NNet import NNetWrapper as NNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ws3_helper import loadTrainExamples\n",
    "from ws3_helper import save_model_checkpoint\n",
    "from ws3_helper import load_model_checkpoint\n",
    "from ws3_helper import OthelloGame\n",
    "from ws3_helper import RandomPlayer\n",
    "from ws3_helper import OthelloNNet\n",
    "from ws3_helper import ValueNetwork\n",
    "from ws3_helper import ValueBasedPlayer\n",
    "from ws3_helper import PolicyNetwork\n",
    "from ws3_helper import PolicyBasedPlayer\n",
    "from ws3_helper import MonteCarlo\n",
    "from ws3_helper import MonteCarloBasedPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Go with Neural Networks and Monte Carlo Tree Search\n",
    "\n",
    "- How did AI master the ancient game of Go?\n",
    "- Discover the power of self-play and deep learning.\n",
    "- See how Monte Carlo Tree Search (MCTS) guides decision-making in real time.\n",
    "- Learn how AlphaZero taught itself to dominate Go without any human knowledge.\n",
    "\n",
    "<img src=\"../images/go_game.JPG\" alt=\"\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/alphago_paper.png\" alt=\"\" width=\"600\"/>\n",
    "\n",
    "The AlphaGo Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/alphazero_paper.png\" alt=\"\" width=\"600\"/>\n",
    "\n",
    "The AlphaZero Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/alphazero0.png\" alt=\"Monte Carlo tree search in AlphaGo\" width=\"600\"/>\n",
    "\n",
    "In Monte-Carlo Tree Search (MCTS), nodes represent game states, and edges represent actions.\n",
    "\n",
    "- **Leaf Node**: A game state that has not been visited yet.\n",
    "  \n",
    "- **Terminal Node**: A node where the game ends (win, loss, or draw).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Terms for Understanding MCTS in AlphaZero\n",
    "\n",
    "- **$Q(s, a)$ (Action Value)**: The estimated value of taking action $a$ in state $s$, updated as simulations progress. Tracks the average value of actions.\n",
    "\n",
    "- **$P(s, a)$ (Prior Probability)**: Probability of selecting action $a$ in state $s$, predicted by the policy network. Guides the tree search based on learned strategies.\n",
    "\n",
    "- **$N(s, a)$ (Visit Count)**: Number of times action $a$ has been taken from state $s$ during MCTS simulations, balancing exploration and exploitation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/alphago_fig3.png\" alt=\"Monte Carlo tree search in AlphaGo\" width=\"600\"/>\n",
    "\n",
    "\n",
    "MCTS in AlphaZero\n",
    "\n",
    "- **(a)**: **Selection**\n",
    "  - Select the edge with the highest score.\n",
    "  - The edge's action value $Q(s, a)$ is combined with a bonus term $u(P)$ which is based on the stored prior probability $P(s, a)$ for that edge.\n",
    "  - The goal is to balance exploration (using $u(P)$) and exploitation (using $Q(s, a)$) when choosing the next action:\n",
    "    $$\n",
    "    u(P) = c_{puct} \\cdot P(s, a) \\cdot \\frac{\\sqrt{\\sum_b N(s, b)}}{1 + N(s, a)}\n",
    "    $$\n",
    "    where $c_{puct}$ is a constant controlling exploration, and $N(s, a)$ is the number of times the edge $(s, a)$ has been visited.\n",
    "\n",
    "- **(b)**: **Leaf Node Expansion**\n",
    "  - When a leaf node is reached, it may be expanded by adding a new node to the tree.\n",
    "  - The policy network $p_\\sigma$ processes the new node, generating a set of action probabilities.\n",
    "  - These probabilities are stored as prior probabilities $P(s, a)$ for each possible action from that node, guiding future simulations.\n",
    "\n",
    "- **(c)**: **Leaf Node Evaluation**\n",
    "    - The value network $v_\\theta$ evaluates the current state, producing a value estimate for the player at the node.\n",
    "\n",
    "- **(d)**: **Action Value Update**\n",
    "  - Update $Q(s, a)$ to reflect the mean value of all evaluations $v_\\theta$ in the subtree below that action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise 1: MCTS planner\n",
    "\n",
    "In building the MCTS planner, we will focus on the action selection part, particularly the objective function used. MCTS will use a combination of the current action-value function $Q$ and the policy prior as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{a}{\\operatorname{argmax}} (Q(s_t, a)+u(s_t, a))\n",
    "\\end{equation}\n",
    "\n",
    "with $u(s_t, a)=c_{puct} \\cdot P(s,a) \\cdot \\frac{\\sqrt{\\sum_b N(s,b)}}{1+N(s,a)}$. This effectively implements an Upper Confidence bound applied to Trees (UCT). UCT balances exploration and exploitation by taking the values stored from the MCTS into account. The trade-off is parametrized by $c_{puct}$.\n",
    "\n",
    "**Note**: Polynomial Upper Confidence Trees (PUCT) is the technical term for the alorithm below in which we sequentially run MCTS and store/use information from previous runs to explore and find optimal actions).\n",
    "\n",
    "<br>\n",
    "\n",
    "**Exercise**:\n",
    "* Finish the MCTS planner by using UCT to select actions to build the tree.\n",
    "* Deploy the MCTS planner to build a tree search for a given board position, producing value estimates and action counts for that position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Initialization and Attribute Definitions\n",
    "\n",
    "The `MCTS` class is initialized with three main arguments: `game`, `nnet`, and `args`. These initialize the game environment, the neural network for evaluating game states, and various parameters for running the Monte Carlo Tree Search (MCTS) algorithm. Additionally, several dictionaries (`Qsa`, `Nsa`, `Ns`, `Ps`, `Es`, `Vs`) are defined to store information about the search tree.\n",
    "\n",
    "```python\n",
    "class MCTS():\n",
    "    def __init__(self, game, nnet, args):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          game: OthelloGame instance\n",
    "          nnet: OthelloNet instance\n",
    "        \"\"\"\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.args = args\n",
    "        self.Qsa = {}  # Stores Q values for s,a\n",
    "        self.Nsa = {}  # Stores #times edge s,a was visited\n",
    "        self.Ns = {}  # Stores #times board s was visited\n",
    "        self.Ps = {}  # Stores initial policy (returned by neural net)\n",
    "        self.Es = {}  # Stores game.getGameEnded ended for board s\n",
    "        self.Vs = {}  # Stores game.getValidMoves for board s\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Search Method and Terminal State Check\n",
    "\n",
    "`search` function: Performs one iteration of MCTS.\n",
    "\n",
    "- Check if the current state `s` is terminal (`self.Es`).\n",
    "  - If terminal, return the game outcome (negated value).\n",
    "  - If not terminal, recursively call `search` on the next state.\n",
    "  - Recursion continues until a leaf node or terminal state is reached.\n",
    "\n",
    "```python\n",
    "    def search(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        Perform one iteration of MCTS.\n",
    "        Args:\n",
    "          canonicalBoard: Canonical Board of size n x n.\n",
    "        Returns:\n",
    "          float: The negative value of the current canonical board state.\n",
    "        \"\"\"\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "\n",
    "        if s not in self.Es:\n",
    "            self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
    "        if self.Es[s] != 0:\n",
    "            # Terminal node\n",
    "            return -self.Es[s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Leaf Node Detection and Neural Network Prediction\n",
    "\n",
    "If the state `s` is not a terminal state, the next step is to check whether the state is a leaf node. If it is, the neural network is called to predict the policy (`Ps`) and the value (`v`) of the state. The valid moves are then masked in the policy to eliminate invalid actions.\n",
    "\n",
    "```python\n",
    "        if s not in self.Ps:\n",
    "            # Leaf node\n",
    "            self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "            valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "            self.Ps[s] = self.Ps[s] * valids  # Masking invalid moves\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s  # Renormalize\n",
    "            else:\n",
    "                # If all valid moves were masked make all valid moves equally probable\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "            self.Vs[s] = valids\n",
    "            self.Ns[s] = 0\n",
    "\n",
    "            return -v\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Action Selection Using Upper Confidence Bound\n",
    "\n",
    "After handling the leaf nodes, the algorithm selects the next action to explore based on the Upper Confidence Bound (UCB). This value balances exploration (selecting actions with fewer visits) and exploitation (selecting actions with higher Q-values). The action with the highest UCB is selected.\n",
    "\n",
    "```python\n",
    "        valids = self.Vs[s]\n",
    "        cur_best = -float(\"inf\")\n",
    "        best_act = -1\n",
    "\n",
    "        # Pick the action with the highest upper confidence bound\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                if (s, a) in self.Qsa:\n",
    "                    u = self.Qsa[(s, a)] + self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s]) / (\n",
    "                        1 + self.Nsa[(s, a)]\n",
    "                    )\n",
    "                else:\n",
    "                    u = self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s] + 1e-8)\n",
    "\n",
    "                if u > cur_best:\n",
    "                    cur_best = u\n",
    "                    best_act = a\n",
    "\n",
    "        a = best_act\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Recursion and Value Propagation\n",
    "\n",
    "Once the best action is chosen, the algorithm moves to the next state by calling the game environment. The search continues recursively from this next state. Once a value `v` is returned from the deeper searches, the Q-values and visit counts are updated along the path.\n",
    "\n",
    "```python\n",
    "        next_s, next_player = self.game.getNextState(canonicalBoard, 1, a)\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_player)\n",
    "\n",
    "        v = self.search(next_s)\n",
    "\n",
    "        if (s, a) in self.Qsa:\n",
    "            self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n",
    "            self.Nsa[(s, a)] += 1\n",
    "        else:\n",
    "            self.Qsa[(s, a)] = v\n",
    "            self.Nsa[(s, a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        return -v\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "class MCTS:\n",
    "\n",
    "    def __init__(self, game, nnet, args):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          game: OthelloGame instance\n",
    "            Instance of the OthelloGame class above;\n",
    "          nnet: OthelloNet instance\n",
    "            Instance of the OthelloNNet class above;\n",
    "          args: dictionary\n",
    "            Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "            arena, checkpointing, and neural network parameters:\n",
    "            learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "            num_channels: 512\n",
    "        \"\"\"\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.args = args\n",
    "        self.Qsa = {}  # Stores Q values for s,a (as defined in the paper)\n",
    "        self.Nsa = {}  # Stores #times edge s,a was visited\n",
    "        self.Ns = {}  # Stores #times board s was visited\n",
    "        self.Ps = {}  # Stores initial policy (returned by neural net)\n",
    "        self.Es = {}  # Stores game.getGameEnded ended for board s\n",
    "        self.Vs = {}  # Stores game.getValidMoves for board s\n",
    "\n",
    "    def search(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        Perform one iteration of MCTS.\n",
    "\n",
    "        It is recursively called till a leaf node is found. The action chosen at\n",
    "        each node is one that has the maximum upper confidence bound.\n",
    "        Once a leaf node is found, the neural network is called to return an\n",
    "        initial policy P and a value v for the state. This value is propagated\n",
    "        up the search path. In case the leaf node is a terminal state, the\n",
    "        outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n",
    "        updated.\n",
    "        NOTE: the return values are the negative of the value of the current\n",
    "        state. This is done since v is in [-1,1] and if v is the value of a\n",
    "        state for the current player, then its value is -v for the other player.\n",
    "\n",
    "        Args:\n",
    "          canonicalBoard: np.ndarray\n",
    "            Canonical Board of size n x n [6x6 in this case]\n",
    "\n",
    "        Returns:\n",
    "            v: Float\n",
    "              The negative of the value of the current canonicalBoard\n",
    "        \"\"\"\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "\n",
    "        if s not in self.Es:\n",
    "            self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
    "        if self.Es[s] != 0:\n",
    "            # Terminal node\n",
    "            return -self.Es[s]\n",
    "\n",
    "        if s not in self.Ps:\n",
    "            # Leaf node\n",
    "            self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "            valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "            self.Ps[s] = self.Ps[s] * valids  # Masking invalid moves\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s  # Renormalize\n",
    "            else:\n",
    "                # If all valid moves were masked make all valid moves equally probable\n",
    "                # NB! All valid moves may be masked if either your NNet architecture is\n",
    "                # insufficient or you've get overfitting or something else.\n",
    "                # If you have got dozens or hundreds of these messages you should\n",
    "                # pay attention to your NNet and/or training process.\n",
    "                log = logging.getLogger(__name__)\n",
    "                log.error(\"All valid moves were masked, doing a workaround.\")\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "            self.Vs[s] = valids\n",
    "            self.Ns[s] = 0\n",
    "\n",
    "            return -v\n",
    "\n",
    "        valids = self.Vs[s]\n",
    "        cur_best = -float(\"inf\")\n",
    "        best_act = -1\n",
    "\n",
    "        # Pick the action with the highest upper confidence bound\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                if (s, a) in self.Qsa:\n",
    "                    u = self.Qsa[(s, a)] + self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s]) / (\n",
    "                        1 + self.Nsa[(s, a)]\n",
    "                    )\n",
    "                else:\n",
    "                    u = self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s] + 1e-8)\n",
    "\n",
    "                if u > cur_best:\n",
    "                    cur_best = u\n",
    "                    best_act = a\n",
    "\n",
    "        a = best_act\n",
    "        next_s, next_player = self.game.getNextState(canonicalBoard, 1, a)\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_player)\n",
    "\n",
    "        v = self.search(next_s)\n",
    "\n",
    "        if (s, a) in self.Qsa:\n",
    "            self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n",
    "            self.Nsa[(s, a)] += 1\n",
    "\n",
    "        else:\n",
    "            self.Qsa[(s, a)] = v\n",
    "            self.Nsa[(s, a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        return -v\n",
    "\n",
    "    def getNsa(self):\n",
    "        return self.Nsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Use MCTS to play games\n",
    "\n",
    "*Time estimate: ~10 mins*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Learn how to use the results of MCTS to play games.\n",
    "\n",
    "**Exercise:**\n",
    "* Plug the MCTS planner into an agent.\n",
    "* Play games against other agents.\n",
    "* Explore the contributions of prior network, value function, number of simulations/time to play and explore/exploit parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MCTS model from the repository\n",
    "mcts_model_save_name = 'MCTS.pth.tar'\n",
    "path = \"../nma_rl_games/alpha-zero/pretrained_models/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2023 has been set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/repo/nma_rl_games/alpha-zero/othello/pytorch/NNet.py:119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******MCTS player versus random player******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Arena.playGames (1): 100%|██████████| 10/10 [00:17<00:00,  1.79s/it]\n",
      "Arena.playGames (2): 100%|██████████| 10/10 [00:16<00:00,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of games won by player1 = 19, number of games won by player2 = 1, out of 20 games\n",
      "\n",
      "Win rate for player1 over 20 games: 95.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# to_remove solution\n",
    "class MonteCarloTreeSearchBasedPlayer():\n",
    "\n",
    "  def __init__(self, game, nnet, args):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: OthelloGame instance\n",
    "        Instance of the OthelloGame class above;\n",
    "      nnet: OthelloNet instance\n",
    "        Instance of the OthelloNNet class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.nnet = nnet\n",
    "    self.args = args\n",
    "    self.mcts = MCTS(game, nnet, args)\n",
    "\n",
    "  def play(self, canonicalBoard, temp=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      canonicalBoard: np.ndarray\n",
    "        Canonical Board of size n x n [6x6 in this case]\n",
    "      temp: Integer\n",
    "        Signifies if game is in terminal state\n",
    "\n",
    "    Returns:\n",
    "      List of probabilities for all actions if temp is 0\n",
    "      Best action based on max probability otherwise\n",
    "    \"\"\"\n",
    "    for i in range(self.args.numMCTSSims):\n",
    "      self.mcts.search(canonicalBoard)\n",
    "\n",
    "    s = self.game.stringRepresentation(canonicalBoard)\n",
    "    self.Nsa = self.mcts.getNsa()\n",
    "    self.counts = [self.Nsa[(s, a)] if (s, a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
    "\n",
    "    if temp == 0:\n",
    "      bestAs = np.array(np.argwhere(self.counts == np.max(self.counts))).flatten()\n",
    "      bestA = np.random.choice(bestAs)\n",
    "      probs = [0] * len(self.counts)\n",
    "      probs[bestA] = 1\n",
    "      return probs\n",
    "\n",
    "    self.counts = [x ** (1. / temp) for x in self.counts]\n",
    "    self.counts_sum = float(sum(self.counts))\n",
    "    probs = [x / self.counts_sum for x in self.counts]\n",
    "    return np.argmax(probs)\n",
    "\n",
    "  def getActionProb(self, canonicalBoard, temp=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      canonicalBoard: np.ndarray\n",
    "        Canonical Board of size n x n [6x6 in this case]\n",
    "      temp: Integer\n",
    "        Signifies if game is in terminal state\n",
    "\n",
    "    Returns:\n",
    "      action_probs: List\n",
    "        Probability associated with corresponding action\n",
    "    \"\"\"\n",
    "    action_probs = np.zeros((self.game.getActionSize()))\n",
    "    best_action = self.play(canonicalBoard)\n",
    "    action_probs[best_action] = 1\n",
    "\n",
    "    return action_probs\n",
    "\n",
    "\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "rp = RandomPlayer(game).play  # All players\n",
    "num_games = 20  # Games\n",
    "n1 = NNet(game)  # nnet players\n",
    "n1.load_checkpoint(folder=path, filename=mcts_model_save_name)\n",
    "args1 = dotdict({'numMCTSSims': 50, 'cpuct':1.0})\n",
    "\n",
    "## Uncomment below to check your agent!\n",
    "print('\\n******MCTS player versus random player******')\n",
    "mcts1 = MonteCarloTreeSearchBasedPlayer(game, n1, args1)\n",
    "n1p = lambda x: np.argmax(mcts1.getActionProb(x, temp=0))\n",
    "arena = Arena.Arena(n1p, rp, game, display=OthelloGame.display)\n",
    "MCTS_result = arena.playGames(num_games, verbose=False)\n",
    "print(f\"\\nNumber of games won by player1 = {MCTS_result[0]}, \"\n",
    "      f\"number of games won by player2 = {MCTS_result[1]}, out of {num_games} games\")\n",
    "win_rate_player1 = MCTS_result[0]/num_games\n",
    "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/repo/workshop/../ws3_helper.py:189: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filepath, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2023 has been set.\n",
      "Random seed 2023 has been set.\n"
     ]
    }
   ],
   "source": [
    "# @title Load in trained value and policy networks\n",
    "model_save_name = 'ValueNetwork.pth.tar'\n",
    "path = \"../nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "vnet = ValueNetwork(game)\n",
    "vnet.load_checkpoint(folder=path, filename=model_save_name)\n",
    "\n",
    "model_save_name = 'PolicyNetwork.pth.tar'\n",
    "path = \"../nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "pnet = PolicyNetwork(game)\n",
    "pnet.load_checkpoint(folder=path, filename=model_save_name)\n",
    "\n",
    "# Alternative if the downloading of trained model didn't work (will train the model)\n",
    "if not os.listdir('../nma_rl_games/alpha-zero/pretrained_models/models/'):\n",
    "  path = \"../nma_rl_games/alpha-zero/pretrained_models/data/\"\n",
    "  loaded_games = loadTrainExamples(folder=path, filename='checkpoint_1.pth.tar')\n",
    "\n",
    "  set_seed(seed=SEED)\n",
    "  game = OthelloGame(6)\n",
    "  vnet = ValueNetwork(game)\n",
    "  vnet.train(loaded_games)\n",
    "\n",
    "  set_seed(seed=SEED)\n",
    "  game = OthelloGame(6)\n",
    "  pnet = PolicyNetwork(game)\n",
    "  pnet.train(loaded_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTS player against Value-based player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******MCTS player versus value-based player******\n",
      "Random seed 2023 has been set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Arena.playGames (1): 100%|██████████| 10/10 [00:17<00:00,  1.79s/it]\n",
      "Arena.playGames (2): 100%|██████████| 10/10 [00:17<00:00,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of games won by player1 = 17, number of games won by player2 = 3, out of 20 games\n",
      "\n",
      "Win rate for player1 over 20 games: 85.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n******MCTS player versus value-based player******')\n",
    "set_seed(seed=SEED)\n",
    "vp = ValueBasedPlayer(game, vnet).play  # Value-based player\n",
    "arena = Arena.Arena(n1p, vp, game, display=OthelloGame.display)\n",
    "MC_result = arena.playGames(num_games, verbose=False)\n",
    "\n",
    "print(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n",
    "      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\n",
    "win_rate_player1 = MC_result[0]/num_games\n",
    "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTS player against Policy-based player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******MCTS player versus policy-based player******\n",
      "Random seed 2023 has been set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Arena.playGames (1): 100%|██████████| 10/10 [00:19<00:00,  1.92s/it]\n",
      "Arena.playGames (2): 100%|██████████| 10/10 [00:16<00:00,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of games won by player1 = 20, number of games won by player2 = 0, out of 20 games\n",
      "\n",
      "Win rate for player1 over 20 games: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n******MCTS player versus policy-based player******')\n",
    "set_seed(seed=SEED)\n",
    "pp = PolicyBasedPlayer(game, pnet).play  # Policy-based player\n",
    "arena = Arena.Arena(n1p, pp, game, display=OthelloGame.display)\n",
    "MC_result = arena.playGames(num_games, verbose=False)\n",
    "\n",
    "print(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n",
    "      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\n",
    "win_rate_player1 = MC_result[0]/num_games\n",
    "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTS player against Monte-Carlo player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model_save_name = 'MC.pth.tar'\n",
    "path = \"nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "\n",
    "n2 = NNet(game)  # nNet players\n",
    "n2.load_checkpoint(folder=path, filename=mc_model_save_name)\n",
    "args2 = dotdict({'numMCsims': 10, 'maxRollouts':5, 'maxDepth':5, 'mc_topk': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******MCTS player versus MC player******\n",
      "Random seed 2023 has been set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Arena.playGames (1): 100%|██████████| 10/10 [01:40<00:00, 10.08s/it]\n",
      "Arena.playGames (2): 100%|██████████| 10/10 [01:43<00:00, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of games won by player1 = 16, number of games won by player2 = 4, out of 20 games\n",
      "\n",
      "Win rate for player1 over 20 games: 80.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n******MCTS player versus MC player******')\n",
    "set_seed(seed=SEED)\n",
    "mc = MonteCarloBasedPlayer(game, n2, args2)\n",
    "n2p = lambda x: np.argmax(mc.getActionProb(x))\n",
    "arena = Arena.Arena(n1p, n2p, game, display=OthelloGame.display)\n",
    "MC_result = arena.playGames(num_games, verbose=False)\n",
    "\n",
    "print(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n",
    "      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\n",
    "win_rate_player1 = MC_result[0]/num_games\n",
    "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "In this tutorial, you have learned about players with Monte Carlo Tree Search planner and compared them to random, value-based, policy-based, and Monte-Carlo players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
