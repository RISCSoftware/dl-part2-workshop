{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "We are excited to have you participate in this workshop, designed to bridge the gap between theory and practice in advanced machine learning, focusing on neural networks, transformers, and neurosymbolic AI. Through hands-on coding exercises and theoretical discussions, you will gain both foundational and advanced knowledge, enabling you to tackle real-world problems in fields such as time series forecasting, natural language processing, and computer vision.\n",
    "\n",
    "## Speakers\n",
    "\n",
    "### Evans Ocansey\n",
    "\n",
    "<img src=\"../images/eocansey.jpg\" alt=\"Image of Evans Ocansey\" width=\"200\"/>\n",
    "\n",
    "Evans Doe Ocansey earned his PhD in Mathematics (Distinction) from Johannes Kepler University Linz in 2019, following earlier studies in mathematical sciences. He then worked as a Postdoc at JKUâ€™s Institute for Algebra & Research Institute for Symbolic Computation before joining RISC Software GmbH in 2022 as a Data Scientist, focusing on time series forecasting, machine learning, and deep learning.\n",
    "\n",
    "### Markus Steindl\n",
    "\n",
    "<img src=\"../images/msteindl.jpg\" alt=\"Image of Markus Steindl\" width=\"200\"/>\n",
    "\n",
    "Markus Steindl completed his studies in Technical Physics and Technical Mathematics at Johannes Kepler University Linz, earning his PhD in 2015. Since joining RISC Software GmbH as a data scientist in 2018, he has focused on big data systems, predictive maintenance, NLP components, and deep learning-based computer vision\n",
    "\n",
    "## Setup for Coding\n",
    "\n",
    "For detailed setup instructions, please refer to the [README.md](../README.md).\n",
    "\n",
    "## Workshop Outline\n",
    "\n",
    "The workshop is structured into three parts, covering essential topics in machine learning:\n",
    "\n",
    "**Part 1**\n",
    "  - [1a_torch_tensors.ipynb](1a_torch_tensors.ipynb) - Tensor manipulation in PyTorch.\n",
    "  - [1b_neural_nets.ipynb](1b_neural_nets.ipynb) - Basics of neural networks in PyTorch.\n",
    "\n",
    "**Part 2**\n",
    "  - [2_transformer_self_attention.ipynb](2_transformer_self_attention.ipynb) - Introduction to transformers and self-attention mechanisms.\n",
    "\n",
    "**Part 3**\n",
    "  - [3_neurosymbolic_ai.ipynb](3_neurosymbolic_ai.ipynb) - Neurosymbolic AI concepts and applications.\n",
    "\n",
    "\n",
    "\n",
    "## Resources\n",
    "\n",
    "### Dive into Deep Learning\n",
    "\n",
    "https://d2l.ai/chapter_multilayer-perceptrons/mlp-implementation.html\n",
    "\n",
    "<img src=\"../images/dive1.png\" alt=\"Dive into Deep Learning\" width=\"70%\"/>\n",
    "\n",
    "<img src=\"../images/dive2.png\" alt=\"Dive into Deep Learning\" width=\"70%\"/>\n",
    "\n",
    "### The Illustrated Transformer by Jay Alammar\n",
    "  - [Website](https://jalammar.github.io/illustrated-transformer)\n",
    "  - Images: Some images are modified versions of those from the illustrated transformer\n",
    "    - <img src=\"../images/transformer_positional_encoding_vectors.png\" alt=\"The Illustrate Transformer\" width=\"70%\"/>\n",
    "    - <img src=\"../images/decoder_with_tensors_2.png\" alt=\"The Illustrated Transformer\" width=\"70%\"/>\n",
    "    - <img src=\"../images/self_attention_step_01.png\" alt=\"The Illustrated Transformer\" width=\"70%\"/>\n",
    "    - <img src=\"../images/self_attention_step_02.png\" alt=\"The Illustrated Transformer\" width=\"70%\"/>\n",
    "\n",
    "\n",
    "### Understanding Deep Learning by Simon J. Prince\n",
    " - [Website](https://udlbook.github.io/udlbook/)\n",
    " - [GitHub](https://github.com/udlbook/udlbook/tree/main)\n",
    " - Images: The following images were used\n",
    "    - <img src=\"../images/two-head-self-attention.png\" alt=\"Dive into Deep Learning\" width=\"70%\"/>\n",
    "### TODO: Other Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
