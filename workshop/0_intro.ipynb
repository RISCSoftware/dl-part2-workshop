{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "We are excited to have you participate in this workshop, designed to bridge the gap between theory and practice in advanced machine learning, focusing on neural networks, transformers, and neurosymbolic AI. Through hands-on coding exercises and theoretical discussions, you will gain both foundational and advanced knowledge, enabling you to tackle real-world problems in fields such as time series forecasting, natural language processing, and computer vision.\n",
    "\n",
    "## Speakers\n",
    "\n",
    "### Evans Ocansey\n",
    "\n",
    "<img src=\"../images/eocansey.jpg\" alt=\"Image of Evans Ocansey\" width=\"200\"/>\n",
    "\n",
    "Evans Doe Ocansey earned his PhD in Mathematics (Distinction) from Johannes Kepler University Linz in 2019, following earlier studies in mathematical sciences. He then worked as a Postdoc at JKUâ€™s Institute for Algebra & Research Institute for Symbolic Computation before joining RISC Software GmbH in 2022 as a Data Scientist, focusing on time series forecasting, machine learning, and deep learning.\n",
    "\n",
    "### Markus Steindl\n",
    "\n",
    "<img src=\"../images/msteindl.jpg\" alt=\"Image of Markus Steindl\" width=\"200\"/>\n",
    "\n",
    "Markus Steindl completed his studies in Technical Physics and Technical Mathematics at Johannes Kepler University Linz, earning his PhD in 2015. Since joining RISC Software GmbH as a data scientist in 2018, he has focused on big data systems, predictive maintenance, NLP components, and deep learning-based computer vision\n",
    "\n",
    "## Setup for Coding\n",
    "\n",
    "For detailed setup instructions, please refer to the [README.md](../README.md).\n",
    "\n",
    "## Workshop Outline\n",
    "\n",
    "The workshop is structured into three parts, covering essential topics in machine learning:\n",
    "\n",
    "**Part 1**\n",
    "  - [1a_torch_tensors.ipynb](1a_torch_tensors.ipynb) - Tensor manipulation in PyTorch.\n",
    "  - [1b_neural_nets.ipynb](1b_neural_nets.ipynb) - Basics of neural networks in PyTorch.\n",
    "\n",
    "**Part 2**\n",
    "  - [2_transformer_self_attention.ipynb](2_transformer_self_attention.ipynb) - Introduction to transformers and self-attention mechanisms.\n",
    "\n",
    "**Part 3**\n",
    "  - [3_neurosymbolic_ai.ipynb](3_neurosymbolic_ai.ipynb) - Neurosymbolic AI: Deep Learning and Monte Carlo Tree Search\n",
    "\n",
    "\n",
    "\n",
    "## Resources\n",
    "\n",
    "### Dive into Deep Learning\n",
    "\n",
    "https://d2l.ai/chapter_multilayer-perceptrons/mlp-implementation.html\n",
    "\n",
    "<img src=\"../images/dive1.png\" alt=\"Dive into Deep Learning\" width=\"70%\"/>\n",
    "\n",
    "<img src=\"../images/dive2.png\" alt=\"Dive into Deep Learning\" width=\"70%\"/>\n",
    "\n",
    "### All You Need To Know About LLM Text Generation by Javaid Nabi\n",
    "  - [Website](https://medium.com/@javaid.nabi/all-you-need-to-know-about-llm-text-generation-03b138e0ed19)\n",
    "  - Images:\n",
    "     - Introduction\n",
    "        <p align=\"center\">\n",
    "        <div style=\"display: flex; justify-content: space-around;\">\n",
    "            <img src=\"../images/next-token-prediction.png\" alt=\"Image 1\" style=\"width:40%; margin-top:25px; margin-right:10px; \">\n",
    "            <img src=\"../images/probability-distribution.png\" alt=\"Image 2\" style=\"width:40%; margin-top:25px;\">\n",
    "        </div>\n",
    "    </p>\n",
    "\n",
    "### Attention Is All You Need by Vaswani et al.\n",
    "  - [Website](https://arxiv.org/pdf/1706.03762)\n",
    "  - Image:\n",
    "    - Abstract:\n",
    "        <p align=\"left\">\n",
    "          <img src=\"../images/attention-is-all-you-need.png\" alt=\"Attention Is All You Need\", style=\"width:50%; margin-top:25px;\">\n",
    "        </p>\n",
    "\n",
    "### Pytorch Implementation of Transformers Explained with Comments by Prashant Kumar\n",
    "  - [Website](https://user-images.githubusercontent.com/16246821/79481335-f70d9400-802c-11ea-83f7-6f470fe46196.png)\n",
    "  - [GitHub](https://github.com/pashu123/Transformers)\n",
    "  - Images:\n",
    "    - Introduction:\n",
    "        <p align=\"left\">\n",
    "          <img src=\"../images/transformer-architecture.png\" alt=\"Pytorch Implementation of Transformers Explained with Comments\", style=\"width:50%; margin-top:25px;\">\n",
    "        </p>\n",
    "### The Illustrated Transformer by Jay Alammar\n",
    "  - [Website](https://jalammar.github.io/illustrated-transformer)\n",
    "  - Images: \n",
    "    - Text Preprocessing: Modified version\n",
    "        <p align=\"left\">\n",
    "          <img src=\"../images/transformer_positional_encoding_vectors.png\" alt=\"The Illustrated Transformer\", style=\"width:50%; margin-top:25px;\">\n",
    "        </p>\n",
    "    - Self-attention mechanism - Step 01:\n",
    "        <p align=\"left\">\n",
    "          <img src=\"../images/self_attention_step_01.png\" alt=\"The Illustrated Transformer\", style=\"width:50%; margin-top:25px;\">\n",
    "        </p>\n",
    "    - Self-attention mechanism - Step 02:\n",
    "        <p align=\"left\">\n",
    "          <img src=\"../images/self_attention_step_02.png\" alt=\"The Illustrated Transformer\", style=\"width:50%; margin-top:25px;\">\n",
    "        </p>\n",
    "    - Self-attention mechanism - All Steps:\n",
    "        <p align=\"left\">\n",
    "          <img src=\"../images/self-attention-output.png\" alt=\"The Illustrated Transformer\", style=\"width:50%; height=50%; margin-top:25px;\">\n",
    "        </p>\n",
    "    - Self-attention mechanism - Matrix Computation: Step 1\n",
    "        <p align=\"left\">\n",
    "          <img src=\"../images/self-attention-matrix-calculation.png\" alt=\"The Illustrated Transformer\", style=\"width:50%; height=50%; margin-top:25px;\">\n",
    "        </p>\n",
    "    - Self-attention mechanism - Matrix Computation: Step 2\n",
    "        <p align=\"left\">\n",
    "          <img src=\"../images/self-attention-matrix-calculation-2.png\" alt=\"The Illustrated Transformer\", style=\"width:50%; margin-top:25px;\">\n",
    "        </p>\n",
    "\n",
    "### Understanding Deep Learning by Simon J. Prince\n",
    " - [Website](https://udlbook.github.io/udlbook/)\n",
    " - [GitHub](https://github.com/udlbook/udlbook/tree/main)\n",
    " - Jupyter Notebooks:\n",
    "      - [Self-Attention Exercises](https://github.com/udlbook/udlbook/blob/main/Notebooks/Chap12/12_1_Self_Attention.ipynb)\n",
    "      - [2 Head Self-Attention Exercise](https://github.com/udlbook/udlbook/blob/main/Notebooks/Chap12/12_2_Multihead_Self_Attention.ipynb)\n",
    " - Images:\n",
    "    - A two head self-attention mechanism: \n",
    "      <p align=\"left\">\n",
    "        <img src=\"../images/two-head-self-attention.png\" alt=\"Dive into Deep Learning\", style=\"width:50%; margin-top:25px;\">\n",
    "      </p>\n",
    "\n",
    "\n",
    "### Neuromatch computational neuroscience course\n",
    "\n",
    "- [Website](https://compneuro.neuromatch.io/tutorials/intro.html)\n",
    "- [GitHub 1](https://github.com/raymondchua/nma_rl_games)\n",
    "- [GitHub 2](https://github.com/NeuromatchAcademy/course-content-dl)\n",
    "- [Colab](https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W3D5_ReinforcementLearningForGamesAndDlThinking3/student/W3D5_Tutorial3.ipynb#scrollTo=39Nsmq2xhiK6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
