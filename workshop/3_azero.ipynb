{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import coloredlogs\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "coloredlogs.install(level='INFO')  # Change this to DEBUG to see more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# For DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "# especially if torch modules used.\n",
    "\n",
    "# Inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2023 has been set.\n",
      "WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -> `Change runtime type.`  select `GPU` \n"
     ]
    }
   ],
   "source": [
    "SEED = 2023\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('../nma_rl_games/alpha-zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Arena\n",
    "\n",
    "from utils import *\n",
    "from Game import Game\n",
    "from MCTS import MCTS\n",
    "from NeuralNet import NeuralNet\n",
    "\n",
    "# from othello.OthelloPlayers import *\n",
    "from othello.OthelloLogic import Board\n",
    "# from othello.OthelloGame import OthelloGame\n",
    "from othello.pytorch.NNet import NNetWrapper as NNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict({\n",
    "    'numIters': 1,  # In training, number of iterations = 1000 and num of episodes = 100\n",
    "    'numEps': 1,  # Number of complete self-play games to simulate during a new iteration.\n",
    "    'tempThreshold': 15,  # To control exploration and exploitation\n",
    "    'updateThreshold': 0.6,  # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n",
    "    'maxlenOfQueue': 200,  # Number of game examples to train the neural networks.\n",
    "    'numMCTSSims': 15,  # Number of games moves for MCTS to simulate.\n",
    "    'arenaCompare': 10,  # Number of games to play during arena play to determine if new net will be accepted.\n",
    "    'cpuct': 1,\n",
    "    'maxDepth':5,  # Maximum number of rollouts\n",
    "    'numMCsims': 5,  # Number of monte carlo simulations\n",
    "    'mc_topk': 3,  # Top k actions for monte carlo rollout\n",
    "\n",
    "    'checkpoint': './temp/',\n",
    "    'load_model': False,\n",
    "    'load_folder_file': ('/dev/models/8x100x50','best.pth.tar'),\n",
    "    'numItersForTrainExamplesHistory': 20,\n",
    "\n",
    "    # Define neural network arguments\n",
    "    'lr': 0.001,  # learning rate\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 64,\n",
    "    'device': DEVICE,\n",
    "    'num_channels': 512,\n",
    "})\n",
    "\n",
    "import ws3_helper\n",
    "ws3_helper.args = args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Helper functions from previous tutorials\n",
    "from ws3_helper import loadTrainExamples\n",
    "from ws3_helper import save_model_checkpoint\n",
    "from ws3_helper import load_model_checkpoint\n",
    "from ws3_helper import OthelloGame\n",
    "from ws3_helper import RandomPlayer\n",
    "from ws3_helper import OthelloNNet\n",
    "from ws3_helper import ValueNetwork\n",
    "from ws3_helper import ValueBasedPlayer\n",
    "from ws3_helper import PolicyNetwork\n",
    "from ws3_helper import PolicyBasedPlayer\n",
    "from ws3_helper import MonteCarlo\n",
    "from ws3_helper import MonteCarloBasedPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise 1: MCTS planner\n",
    "\n",
    "In building the MCTS planner, we will focus on the action selection part, particularly the objective function used. MCTS will use a combination of the current action-value function $Q$ and the policy prior as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{a}{\\operatorname{argmax}} (Q(s_t, a)+u(s_t, a))\n",
    "\\end{equation}\n",
    "\n",
    "with $u(s_t, a)=c_{puct} \\cdot P(s,a) \\cdot \\frac{\\sqrt{\\sum_b N(s,b)}}{1+N(s,a)}$. This effectively implements an Upper Confidence bound applied to Trees (UCT). UCT balances exploration and exploitation by taking the values stored from the MCTS into account. The trade-off is parametrized by $c_{puct}$.\n",
    "\n",
    "**Note**: Polynomial Upper Confidence Trees (PUCT) is the technical term for the alorithm below in which we sequentially run MCTS and store/use information from previous runs to explore and find optimal actions).\n",
    "\n",
    "<br>\n",
    "\n",
    "**Exercise**:\n",
    "* Finish the MCTS planner by using UCT to select actions to build the tree.\n",
    "* Deploy the MCTS planner to build a tree search for a given board position, producing value estimates and action counts for that position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Initialization and Attribute Definitions\n",
    "\n",
    "The `MCTS` class is initialized with three main arguments: `game`, `nnet`, and `args`. These initialize the game environment, the neural network for evaluating game states, and various parameters for running the Monte Carlo Tree Search (MCTS) algorithm. Additionally, several dictionaries (`Qsa`, `Nsa`, `Ns`, `Ps`, `Es`, `Vs`) are defined to store information about the search tree.\n",
    "\n",
    "```python\n",
    "class MCTS():\n",
    "    def __init__(self, game, nnet, args):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          game: OthelloGame instance\n",
    "          nnet: OthelloNet instance\n",
    "          args: dictionary\n",
    "            Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "            arena, checkpointing, and neural network parameters:\n",
    "            learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "            num_channels: 512\n",
    "        \"\"\"\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.args = args\n",
    "        self.Qsa = {}  # Stores Q values for s,a (as defined in the paper)\n",
    "        self.Nsa = {}  # Stores #times edge s,a was visited\n",
    "        self.Ns = {}  # Stores #times board s was visited\n",
    "        self.Ps = {}  # Stores initial policy (returned by neural net)\n",
    "        self.Es = {}  # Stores game.getGameEnded ended for board s\n",
    "        self.Vs = {}  # Stores game.getValidMoves for board s\n",
    "```\n",
    "\n",
    "### Part 2: Search Method and Terminal State Check\n",
    "\n",
    "The `search` function performs one iteration of MCTS. The first step is to check whether the current state `s` is terminal by checking `self.Es`. If it's a terminal state, the outcome of the game is returned, and the value is negated.\n",
    "\n",
    "```python\n",
    "    def search(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        Perform one iteration of MCTS.\n",
    "        Args:\n",
    "          canonicalBoard: Canonical Board of size n x n.\n",
    "        \"\"\"\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "\n",
    "        if s not in self.Es:\n",
    "            self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
    "        if self.Es[s] != 0:\n",
    "            # Terminal node\n",
    "            return -self.Es[s]\n",
    "```\n",
    "\n",
    "### Part 3: Leaf Node Detection and Neural Network Prediction\n",
    "\n",
    "If the state `s` is not a terminal state, the next step is to check whether the state is a leaf node. If it is, the neural network is called to predict the policy (`Ps`) and the value (`v`) of the state. The valid moves are then masked in the policy to eliminate invalid actions.\n",
    "\n",
    "```python\n",
    "        if s not in self.Ps:\n",
    "            # Leaf node\n",
    "            self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "            valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "            self.Ps[s] = self.Ps[s] * valids  # Masking invalid moves\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s  # Renormalize\n",
    "            else:\n",
    "                # If all valid moves were masked make all valid moves equally probable\n",
    "                # NB! All valid moves may be masked if either your NNet architecture is\n",
    "                # insufficient or you've get overfitting or something else.\n",
    "                # If you have got dozens or hundreds of these messages you should\n",
    "                # pay attention to your NNet and/or training process.\n",
    "                log = logging.getLogger(__name__)\n",
    "                log.error(\"All valid moves were masked, doing a workaround.\")\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "            self.Vs[s] = valids\n",
    "            self.Ns[s] = 0\n",
    "\n",
    "            return -v\n",
    "```\n",
    "\n",
    "### Part 4: Action Selection Using Upper Confidence Bound\n",
    "\n",
    "After handling the leaf nodes, the algorithm selects the next action to explore based on the Upper Confidence Bound (UCB). This value balances exploration (selecting actions with fewer visits) and exploitation (selecting actions with higher Q-values). The action with the highest UCB is selected.\n",
    "\n",
    "```python\n",
    "        valids = self.Vs[s]\n",
    "        cur_best = -float(\"inf\")\n",
    "        best_act = -1\n",
    "\n",
    "        # Pick the action with the highest upper confidence bound\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                if (s, a) in self.Qsa:\n",
    "                    u = self.Qsa[(s, a)] + self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s]) / (\n",
    "                        1 + self.Nsa[(s, a)]\n",
    "                    )\n",
    "                else:\n",
    "                    u = self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s] + 1e-8)\n",
    "\n",
    "                if u > cur_best:\n",
    "                    cur_best = u\n",
    "                    best_act = a\n",
    "\n",
    "        a = best_act\n",
    "```\n",
    "\n",
    "### Part 5: Recursion and Value Propagation\n",
    "\n",
    "Once the best action is chosen, the algorithm moves to the next state by calling the game environment. The search continues recursively from this next state. Once a value `v` is returned from the deeper searches, the Q-values and visit counts are updated along the path.\n",
    "\n",
    "```python\n",
    "        next_s, next_player = self.game.getNextState(canonicalBoard, 1, a)\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_player)\n",
    "\n",
    "        v = self.search(next_s)\n",
    "\n",
    "        if (s, a) in self.Qsa:\n",
    "            self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n",
    "            self.Nsa[(s, a)] += 1\n",
    "        else:\n",
    "            self.Qsa[(s, a)] = v\n",
    "            self.Nsa[(s, a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        return -v\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "class MCTS:\n",
    "\n",
    "    def __init__(self, game, nnet, args):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          game: OthelloGame instance\n",
    "            Instance of the OthelloGame class above;\n",
    "          nnet: OthelloNet instance\n",
    "            Instance of the OthelloNNet class above;\n",
    "          args: dictionary\n",
    "            Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "            arena, checkpointing, and neural network parameters:\n",
    "            learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "            num_channels: 512\n",
    "        \"\"\"\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.args = args\n",
    "        self.Qsa = {}  # Stores Q values for s,a (as defined in the paper)\n",
    "        self.Nsa = {}  # Stores #times edge s,a was visited\n",
    "        self.Ns = {}  # Stores #times board s was visited\n",
    "        self.Ps = {}  # Stores initial policy (returned by neural net)\n",
    "        self.Es = {}  # Stores game.getGameEnded ended for board s\n",
    "        self.Vs = {}  # Stores game.getValidMoves for board s\n",
    "\n",
    "    def search(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        Perform one iteration of MCTS.\n",
    "\n",
    "        It is recursively called till a leaf node is found. The action chosen at\n",
    "        each node is one that has the maximum upper confidence bound.\n",
    "        Once a leaf node is found, the neural network is called to return an\n",
    "        initial policy P and a value v for the state. This value is propagated\n",
    "        up the search path. In case the leaf node is a terminal state, the\n",
    "        outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n",
    "        updated.\n",
    "        NOTE: the return values are the negative of the value of the current\n",
    "        state. This is done since v is in [-1,1] and if v is the value of a\n",
    "        state for the current player, then its value is -v for the other player.\n",
    "\n",
    "        Args:\n",
    "          canonicalBoard: np.ndarray\n",
    "            Canonical Board of size n x n [6x6 in this case]\n",
    "\n",
    "        Returns:\n",
    "            v: Float\n",
    "              The negative of the value of the current canonicalBoard\n",
    "        \"\"\"\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "\n",
    "        if s not in self.Es:\n",
    "            self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
    "        if self.Es[s] != 0:\n",
    "            # Terminal node\n",
    "            return -self.Es[s]\n",
    "\n",
    "        if s not in self.Ps:\n",
    "            # Leaf node\n",
    "            self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "            valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "            self.Ps[s] = self.Ps[s] * valids  # Masking invalid moves\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s  # Renormalize\n",
    "            else:\n",
    "                # If all valid moves were masked make all valid moves equally probable\n",
    "                # NB! All valid moves may be masked if either your NNet architecture is\n",
    "                # insufficient or you've get overfitting or something else.\n",
    "                # If you have got dozens or hundreds of these messages you should\n",
    "                # pay attention to your NNet and/or training process.\n",
    "                log = logging.getLogger(__name__)\n",
    "                log.error(\"All valid moves were masked, doing a workaround.\")\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "            self.Vs[s] = valids\n",
    "            self.Ns[s] = 0\n",
    "\n",
    "            return -v\n",
    "\n",
    "        valids = self.Vs[s]\n",
    "        cur_best = -float(\"inf\")\n",
    "        best_act = -1\n",
    "\n",
    "        # Pick the action with the highest upper confidence bound\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                if (s, a) in self.Qsa:\n",
    "                    u = self.Qsa[(s, a)] + self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s]) / (\n",
    "                        1 + self.Nsa[(s, a)]\n",
    "                    )\n",
    "                else:\n",
    "                    u = self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s] + 1e-8)\n",
    "\n",
    "                if u > cur_best:\n",
    "                    cur_best = u\n",
    "                    best_act = a\n",
    "\n",
    "        a = best_act\n",
    "        next_s, next_player = self.game.getNextState(canonicalBoard, 1, a)\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_player)\n",
    "\n",
    "        v = self.search(next_s)\n",
    "\n",
    "        if (s, a) in self.Qsa:\n",
    "            self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n",
    "            self.Nsa[(s, a)] += 1\n",
    "\n",
    "        else:\n",
    "            self.Qsa[(s, a)] = v\n",
    "            self.Nsa[(s, a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        return -v\n",
    "\n",
    "    def getNsa(self):\n",
    "        return self.Nsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Use MCTS to play games\n",
    "\n",
    "*Time estimate: ~10 mins*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Learn how to use the results of MCTS to play games.\n",
    "\n",
    "**Exercise:**\n",
    "* Plug the MCTS planner into an agent.\n",
    "* Play games against other agents.\n",
    "* Explore the contributions of prior network, value function, number of simulations/time to play and explore/exploit parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MCTS model from the repository\n",
    "mcts_model_save_name = 'MCTS.pth.tar'\n",
    "path = \"../nma_rl_games/alpha-zero/pretrained_models/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2023 has been set.\n",
      "\n",
      "******MCTS player versus random player******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Arena.playGames (1): 100%|██████████| 10/10 [00:16<00:00,  1.69s/it]\n",
      "Arena.playGames (2): 100%|██████████| 10/10 [00:16<00:00,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of games won by player1 = 19, number of games won by player2 = 1, out of 20 games\n",
      "\n",
      "Win rate for player1 over 20 games: 95.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# to_remove solution\n",
    "class MonteCarloTreeSearchBasedPlayer():\n",
    "\n",
    "  def __init__(self, game, nnet, args):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: OthelloGame instance\n",
    "        Instance of the OthelloGame class above;\n",
    "      nnet: OthelloNet instance\n",
    "        Instance of the OthelloNNet class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.nnet = nnet\n",
    "    self.args = args\n",
    "    self.mcts = MCTS(game, nnet, args)\n",
    "\n",
    "  def play(self, canonicalBoard, temp=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      canonicalBoard: np.ndarray\n",
    "        Canonical Board of size n x n [6x6 in this case]\n",
    "      temp: Integer\n",
    "        Signifies if game is in terminal state\n",
    "\n",
    "    Returns:\n",
    "      List of probabilities for all actions if temp is 0\n",
    "      Best action based on max probability otherwise\n",
    "    \"\"\"\n",
    "    for i in range(self.args.numMCTSSims):\n",
    "      self.mcts.search(canonicalBoard)\n",
    "\n",
    "    s = self.game.stringRepresentation(canonicalBoard)\n",
    "    self.Nsa = self.mcts.getNsa()\n",
    "    self.counts = [self.Nsa[(s, a)] if (s, a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
    "\n",
    "    if temp == 0:\n",
    "      bestAs = np.array(np.argwhere(self.counts == np.max(self.counts))).flatten()\n",
    "      bestA = np.random.choice(bestAs)\n",
    "      probs = [0] * len(self.counts)\n",
    "      probs[bestA] = 1\n",
    "      return probs\n",
    "\n",
    "    self.counts = [x ** (1. / temp) for x in self.counts]\n",
    "    self.counts_sum = float(sum(self.counts))\n",
    "    probs = [x / self.counts_sum for x in self.counts]\n",
    "    return np.argmax(probs)\n",
    "\n",
    "  def getActionProb(self, canonicalBoard, temp=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      canonicalBoard: np.ndarray\n",
    "        Canonical Board of size n x n [6x6 in this case]\n",
    "      temp: Integer\n",
    "        Signifies if game is in terminal state\n",
    "\n",
    "    Returns:\n",
    "      action_probs: List\n",
    "        Probability associated with corresponding action\n",
    "    \"\"\"\n",
    "    action_probs = np.zeros((self.game.getActionSize()))\n",
    "    best_action = self.play(canonicalBoard)\n",
    "    action_probs[best_action] = 1\n",
    "\n",
    "    return action_probs\n",
    "\n",
    "\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "rp = RandomPlayer(game).play  # All players\n",
    "num_games = 20  # Games\n",
    "n1 = NNet(game)  # nnet players\n",
    "n1.load_checkpoint(folder=path, filename=mcts_model_save_name)\n",
    "args1 = dotdict({'numMCTSSims': 50, 'cpuct':1.0})\n",
    "\n",
    "## Uncomment below to check your agent!\n",
    "print('\\n******MCTS player versus random player******')\n",
    "mcts1 = MonteCarloTreeSearchBasedPlayer(game, n1, args1)\n",
    "n1p = lambda x: np.argmax(mcts1.getActionProb(x, temp=0))\n",
    "arena = Arena.Arena(n1p, rp, game, display=OthelloGame.display)\n",
    "MCTS_result = arena.playGames(num_games, verbose=False)\n",
    "print(f\"\\nNumber of games won by player1 = {MCTS_result[0]}, \"\n",
    "      f\"number of games won by player2 = {MCTS_result[1]}, out of {num_games} games\")\n",
    "win_rate_player1 = MCTS_result[0]/num_games\n",
    "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load in trained value and policy networks\n",
    "model_save_name = 'ValueNetwork.pth.tar'\n",
    "path = \"../nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "vnet = ValueNetwork(game)\n",
    "vnet.load_checkpoint(folder=path, filename=model_save_name)\n",
    "\n",
    "model_save_name = 'PolicyNetwork.pth.tar'\n",
    "path = \"../nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "pnet = PolicyNetwork(game)\n",
    "pnet.load_checkpoint(folder=path, filename=model_save_name)\n",
    "\n",
    "# Alternative if the downloading of trained model didn't work (will train the model)\n",
    "if not os.listdir('../nma_rl_games/alpha-zero/pretrained_models/models/'):\n",
    "  path = \"../nma_rl_games/alpha-zero/pretrained_models/data/\"\n",
    "  loaded_games = loadTrainExamples(folder=path, filename='checkpoint_1.pth.tar')\n",
    "\n",
    "  set_seed(seed=SEED)\n",
    "  game = OthelloGame(6)\n",
    "  vnet = ValueNetwork(game)\n",
    "  vnet.train(loaded_games)\n",
    "\n",
    "  set_seed(seed=SEED)\n",
    "  game = OthelloGame(6)\n",
    "  pnet = PolicyNetwork(game)\n",
    "  pnet.train(loaded_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTS player against Value-based player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n******MCTS player versus value-based player******')\n",
    "set_seed(seed=SEED)\n",
    "vp = ValueBasedPlayer(game, vnet).play  # Value-based player\n",
    "arena = Arena.Arena(n1p, vp, game, display=OthelloGame.display)\n",
    "MC_result = arena.playGames(num_games, verbose=False)\n",
    "\n",
    "print(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n",
    "      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\n",
    "win_rate_player1 = MC_result[0]/num_games\n",
    "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTS player against Policy-based player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n******MCTS player versus policy-based player******')\n",
    "set_seed(seed=SEED)\n",
    "pp = PolicyBasedPlayer(game, pnet).play  # Policy-based player\n",
    "arena = Arena.Arena(n1p, pp, game, display=OthelloGame.display)\n",
    "MC_result = arena.playGames(num_games, verbose=False)\n",
    "\n",
    "print(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n",
    "      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\n",
    "win_rate_player1 = MC_result[0]/num_games\n",
    "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTS player against Monte-Carlo player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model_save_name = 'MC.pth.tar'\n",
    "path = \"nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "\n",
    "n2 = NNet(game)  # nNet players\n",
    "n2.load_checkpoint(folder=path, filename=mc_model_save_name)\n",
    "args2 = dotdict({'numMCsims': 10, 'maxRollouts':5, 'maxDepth':5, 'mc_topk': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n******MCTS player versus MC player******')\n",
    "set_seed(seed=SEED)\n",
    "mc = MonteCarloBasedPlayer(game, n2, args2)\n",
    "n2p = lambda x: np.argmax(mc.getActionProb(x))\n",
    "arena = Arena.Arena(n1p, n2p, game, display=OthelloGame.display)\n",
    "MC_result = arena.playGames(num_games, verbose=False)\n",
    "\n",
    "print(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n",
    "      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\n",
    "win_rate_player1 = MC_result[0]/num_games\n",
    "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "In this tutorial, you have learned about players with Monte Carlo Tree Search planner and compared them to random, value-based, policy-based, and Monte-Carlo players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
