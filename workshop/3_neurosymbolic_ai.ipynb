{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks and Monte Carlo Tree Search\n",
    "\n",
    "- How did AI master the ancient game of Go?\n",
    "- Discover the power of self-play and deep learning.\n",
    "- See how Monte Carlo Tree Search (MCTS) guides decision-making in real time.\n",
    "- Learn how AlphaZero taught itself to dominate Go without any human knowledge.\n",
    "\n",
    "<img src=\"../images/go_game.JPG\" alt=\"\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The AlphaGo Algorithm**\n",
    "- Combines neural networks with MCTS for decision-making.\n",
    "- Trained using both human expert games and self-play.\n",
    "- Uses policy and value networks to evaluate board states and actions.\n",
    "\n",
    "<img src=\"../images/alphago_paper.png\" alt=\"\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The AlphaZero Algorithm**\n",
    "\n",
    "- Learns from self-play without human data.\n",
    "- Minor alterations of AlphaGo\n",
    "\n",
    "<img src=\"../images/alphazero_paper.png\" alt=\"\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/alphazero0.png\" alt=\"Monte Carlo tree search in AlphaZero\" width=\"1400\"/>\n",
    "<!-- source: https://sebastianbodenstein.com/post/alphazero/ -->\n",
    "\n",
    "In Monte-Carlo Tree Search (MCTS), nodes represent game states, and edges represent actions.\n",
    "\n",
    "- **Root Node**: The current game state from which the MCTS begins exploration.\n",
    "- **Leaf Node**: A future game state that has not been visited yet.\n",
    "- **Terminal Node**: A node where the game ends (win, loss, or draw).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key Terms for Understanding MCTS in AlphaZero\n",
    "\n",
    "- **Game State** $s$\n",
    "\n",
    "- **Action** $a$\n",
    "\n",
    "- **Action Value** $Q(s, a)$: The estimated value of taking action $a$ in state $s$.\n",
    "\n",
    "- **Prior Probability** $P(s, a)$: Probability of selecting action $a$ in state $s$, predicted by the policy network $p_\\sigma$.\n",
    "\n",
    "- **Visit Count** $N(s, a)$: Number of times action $a$ has been taken from state $s$.\n",
    "\n",
    "- **Policy Network** $p_\\sigma$\n",
    "\n",
    "- **Value Network** $v_\\theta$: Predicts the probability of winning from a given game state. It outputs a scalar value between -1 and 1, where:\n",
    "  - $1$ indicates a likely win,\n",
    "  - $-1$ indicates a likely loss,\n",
    "  - $0$ indicates a likely draw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/alphago_fig3.png\" alt=\"Monte Carlo tree search in AlphaGo\" width=\"1400\"/>\n",
    "\n",
    "\n",
    "MCTS in AlphaZero\n",
    "\n",
    "- **(a)**: **Selection**\n",
    "  - The edge's action value $Q(s, a)$ is combined with a bonus term $u(P)$ which is based on the stored prior probability $P(s, a)$ for that edge:\n",
    "    $$\n",
    "    u(P) = c_{puct} \\cdot P(s, a) \\cdot \\frac{\\sqrt{\\sum_b N(s, b)}}{1 + N(s, a)}\n",
    "    $$\n",
    "    where $c_{puct}$ is a constant controlling exploration.\n",
    "  - Select the edge with the highest score.\n",
    "  - Repeat until a leaf node is reached.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/alphago_fig3.png\" alt=\"Monte Carlo tree search in AlphaGo\" width=\"1400\"/>\n",
    "\n",
    "\n",
    "MCTS in AlphaZero\n",
    "\n",
    "- **(b)**: **Leaf Node Expansion**\n",
    "  - When a non-terminal leaf node $s$ is reached, the policy network $p_\\sigma$ generates prior probabilities $P(s, a)$.\n",
    "\n",
    "- **(c)**: **Leaf Node Evaluation**\n",
    "    - The value network $v_\\theta$ evaluates the state of the leaf node.\n",
    "    - If the leaf node is terminal, its value is 1 for a win, -1 for a loss, and 0 for a draw.\n",
    "\n",
    "- **(d)**: **Action Value Update**\n",
    "  - Update $N(s, a)$.\n",
    "  - Update $Q(s, a)$ to reflect the mean value of all evaluations $v_\\theta$ in the subtree below that action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- After running **N MCTS searches**, each action at the root node has a visit count $N(s, a)$ representing how often that action was selected from state $s$.\n",
    "- The **action probabilities** are computed using a temperature parameter $T$ to adjust exploration:\n",
    "  $$\n",
    "  \\pi(a | s) = \\frac{N(s, a)^{1/T}}{\\sum_{b} N(s, b)^{1/T}}\n",
    "  $$\n",
    "- For **high $T$**, the probabilities are more evenly distributed, encouraging exploration.\n",
    "- For **low $T$**, the action with the highest visit count dominates, focusing on exploitation.\n",
    "- The resulting probability distribution $\\pi(a | s)$ guides the policy network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Playing Othello with AlphaZero\n",
    "\n",
    "<img src=\"../images/oth3.png\" alt=\"Othello\" width=\"400\"/><img src=\"../images/oth4.png\" alt=\"Othello\" width=\"408\"/><img src=\"../images/oth5.png\" alt=\"Othello\" width=\"408\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import coloredlogs\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "coloredlogs.install(level='INFO')  # Change this to DEBUG to see more info.\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../nma_rl_games/alpha-zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2023 has been set.\n",
      "WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -> `Change runtime type.`  select `GPU` \n"
     ]
    }
   ],
   "source": [
    "from ws3_helper import set_seed\n",
    "from ws3_helper import seed_worker\n",
    "from ws3_helper import set_device\n",
    "\n",
    "SEED = 2023\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import Arena\n",
    "\n",
    "from utils import *\n",
    "from Game import Game\n",
    "from MCTS import MCTS\n",
    "from NeuralNet import NeuralNet\n",
    "\n",
    "# from othello.OthelloPlayers import *\n",
    "from othello.OthelloLogic import Board\n",
    "# from othello.OthelloGame import OthelloGame\n",
    "from othello.pytorch.NNet import NNetWrapper as NNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from ws3_helper import loadTrainExamples\n",
    "from ws3_helper import save_model_checkpoint\n",
    "from ws3_helper import load_model_checkpoint\n",
    "from ws3_helper import OthelloGame\n",
    "from ws3_helper import RandomPlayer\n",
    "from ws3_helper import OthelloNNet\n",
    "from ws3_helper import ValueNetwork\n",
    "from ws3_helper import ValueBasedPlayer\n",
    "from ws3_helper import PolicyNetwork\n",
    "from ws3_helper import PolicyBasedPlayer\n",
    "from ws3_helper import MonteCarlo\n",
    "from ws3_helper import MonteCarloBasedPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class OthelloNNet(nn.Module):\n",
    "    def __init__(self, game, args):\n",
    "        self.board_x, self.board_y = game.getBoardSize()\n",
    "        self.action_size = game.getActionSize()\n",
    "        self.args = args\n",
    "\n",
    "        super(OthelloNNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=args.num_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=args.num_channels, out_channels=args.num_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=args.num_channels, out_channels=args.num_channels, kernel_size=3, stride=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=args.num_channels, out_channels=args.num_channels, kernel_size=3, stride=1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=args.num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=args.num_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=args.num_channels)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=args.num_channels)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=args.num_channels * (self.board_x - 4) * (self.board_y - 4), out_features=1024)\n",
    "        self.fc_bn1 = nn.BatchNorm1d(num_features=1024)\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=512)\n",
    "        self.fc_bn2 = nn.BatchNorm1d(num_features=512)\n",
    "\n",
    "        self.fc3 = nn.Linear(in_features=512, out_features=self.action_size)\n",
    "\n",
    "        self.fc4 = nn.Linear(in_features=512, out_features=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "\n",
    "@d2l.add_to_class(OthelloNNet)\n",
    "def forward(self, s):\n",
    "    s = s.view(-1, 1, self.board_x, self.board_y)  # batch_size x 1 x board_x x board_y\n",
    "    s = F.relu(self.bn1(self.conv1(s)))  # batch_size x num_channels x board_x x board_y\n",
    "    s = F.relu(self.bn2(self.conv2(s)))  # batch_size x num_channels x board_x x board_y\n",
    "    s = F.relu(self.bn3(self.conv3(s)))  # batch_size x num_channels x (board_x-2) x (board_y-2)\n",
    "    s = F.relu(self.bn4(self.conv4(s)))  # batch_size x num_channels x (board_x-4) x (board_y-4)\n",
    "    s = s.view(-1, self.args.num_channels * (self.board_x - 4) * (self.board_y - 4))\n",
    "\n",
    "    s = F.dropout(\n",
    "        F.relu(self.fc_bn1(self.fc1(s))), p=self.args.dropout, training=self.training\n",
    "    )  # batch_size x 1024\n",
    "    s = F.dropout(F.relu(self.fc_bn2(self.fc2(s))), p=self.args.dropout, training=self.training)  # batch_size x 512\n",
    "\n",
    "    pi = self.fc3(s)  # batch_size x action_size\n",
    "    v = self.fc4(s)  # batch_size x 1\n",
    "\n",
    "    return F.log_softmax(pi, dim=1), torch.tanh(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MCTS\n",
    "\n",
    "### Part 1: Initialization and Attribute Definitions\n",
    "\n",
    "The `MCTS` class is initialized with three main arguments: `game`, `nnet`, and `args`. These initialize the game environment, the neural network for evaluating game states, and various parameters for running the Monte Carlo Tree Search (MCTS) algorithm. Additionally, several dictionaries (`Qsa`, `Nsa`, `Ns`, `Ps`, `Es`, `Vs`) are defined to store information about the search tree.\n",
    "\n",
    "```python\n",
    "class MCTS():\n",
    "    def __init__(self, game, nnet, args):\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.args = args\n",
    "        self.Qsa = {}  # Stores Q values for s,a\n",
    "        self.Nsa = {}  # Stores #times edge s,a was visited\n",
    "        self.Ns = {}  # Stores #times board s was visited\n",
    "        self.Ps = {}  # Stores initial policy (returned by neural net)\n",
    "        self.Es = {}  # Stores game.getGameEnded ended for board s\n",
    "        self.Vs = {}  # Stores game.getValidMoves for board s\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part 2: Search Method and Terminal State Check\n",
    "\n",
    "`search` function: Performs one iteration of MCTS.\n",
    "\n",
    "- Check if the current state `s` is terminal (`self.Es`).\n",
    "- If terminal, return the game outcome (negated value).\n",
    "- If not terminal, recursively call `search` on the next state.\n",
    "- Recursion continues until a leaf node or terminal state is reached.\n",
    "\n",
    "```python\n",
    "    def search(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        Perform one iteration of MCTS.\n",
    "        Args:\n",
    "          canonicalBoard: Canonical Board of size n x n.\n",
    "        Returns:\n",
    "          float: The negative value of the current canonical board state.\n",
    "        \"\"\"\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "\n",
    "        if s not in self.Es:\n",
    "            self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
    "        if self.Es[s] != 0:\n",
    "            # Terminal node\n",
    "            return -self.Es[s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part 3: Leaf Node Detection and Neural Network Prediction\n",
    "\n",
    "If the state `s` is not a terminal state, the next step is to check whether the state is a leaf node. If it is, the neural network is called to predict the policy (`Ps`) and the value (`v`) of the state. The valid moves are then masked in the policy to eliminate invalid actions.\n",
    "\n",
    "```python\n",
    "        if s not in self.Ps:  # Leaf node\n",
    "            self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "            valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "            self.Ps[s] = self.Ps[s] * valids  # Masking invalid moves\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s  # Renormalize\n",
    "            else:\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])  # all valid moves equally probable\n",
    "\n",
    "            self.Vs[s] = valids\n",
    "            self.Ns[s] = 0\n",
    "\n",
    "            return -v\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part 4: Action Selection Using Upper Confidence Bound\n",
    "\n",
    "After handling the leaf nodes, the algorithm selects the next action to explore based on the Upper Confidence Bound (UCB). This value balances exploration (selecting actions with fewer visits) and exploitation (selecting actions with higher Q-values). The action with the highest UCB is selected.\n",
    "\n",
    "```python\n",
    "        valids = self.Vs[s]\n",
    "        cur_best = -float(\"inf\")\n",
    "        best_act = -1\n",
    "```\n",
    "```python\n",
    "        # Pick the action with the highest upper confidence bound\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                if (s, a) in self.Qsa:\n",
    "                    u = self.Qsa[(s, a)] + self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s]) / (\n",
    "                        1 + self.Nsa[(s, a)]\n",
    "                    )\n",
    "                else:\n",
    "                    u = self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s] + 1e-8)\n",
    "\n",
    "                if u > cur_best:\n",
    "                    cur_best = u\n",
    "                    best_act = a\n",
    "\n",
    "        a = best_act\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part 5: Recursion and Value Propagation\n",
    "\n",
    "Once the best action is chosen, the algorithm moves to the next state by calling the game environment. The search continues recursively from this next state. Once a value `v` is returned from the deeper searches, the Q-values and visit counts are updated along the path.\n",
    "\n",
    "```python\n",
    "        next_s, next_player = self.game.getNextState(canonicalBoard, 1, a)\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_player)\n",
    "\n",
    "        v = self.search(next_s)\n",
    "\n",
    "        if (s, a) in self.Qsa:\n",
    "            self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n",
    "            self.Nsa[(s, a)] += 1\n",
    "        else:\n",
    "            self.Qsa[(s, a)] = v\n",
    "            self.Nsa[(s, a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        return -v \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### All parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "\n",
    "    def __init__(self, game, nnet, args):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          game: OthelloGame instance\n",
    "            Instance of the OthelloGame class above;\n",
    "          nnet: OthelloNet instance\n",
    "            Instance of the OthelloNNet class above;\n",
    "          args: dictionary\n",
    "            Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "            arena, checkpointing, and neural network parameters:\n",
    "            learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "            num_channels: 512\n",
    "        \"\"\"\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.args = args\n",
    "        self.Qsa = {}  # Stores Q values for s,a (as defined in the paper)\n",
    "        self.Nsa = {}  # Stores #times edge s,a was visited\n",
    "        self.Ns = {}  # Stores #times board s was visited\n",
    "        self.Ps = {}  # Stores initial policy (returned by neural net)\n",
    "        self.Es = {}  # Stores game.getGameEnded ended for board s\n",
    "        self.Vs = {}  # Stores game.getValidMoves for board s\n",
    "\n",
    "    def search(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        Perform one iteration of MCTS.\n",
    "\n",
    "        It is recursively called till a leaf node is found. The action chosen at\n",
    "        each node is one that has the maximum upper confidence bound.\n",
    "        Once a leaf node is found, the neural network is called to return an\n",
    "        initial policy P and a value v for the state. This value is propagated\n",
    "        up the search path. In case the leaf node is a terminal state, the\n",
    "        outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n",
    "        updated.\n",
    "        NOTE: the return values are the negative of the value of the current\n",
    "        state. This is done since v is in [-1,1] and if v is the value of a\n",
    "        state for the current player, then its value is -v for the other player.\n",
    "\n",
    "        Args:\n",
    "          canonicalBoard: np.ndarray\n",
    "            Canonical Board of size n x n [6x6 in this case]\n",
    "\n",
    "        Returns:\n",
    "            v: Float\n",
    "              The negative of the value of the current canonicalBoard\n",
    "        \"\"\"\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "\n",
    "        if s not in self.Es:\n",
    "            self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
    "        if self.Es[s] != 0:\n",
    "            # Terminal node\n",
    "            return -self.Es[s]\n",
    "\n",
    "        if s not in self.Ps:\n",
    "            # Leaf node\n",
    "            self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "            valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "            self.Ps[s] = self.Ps[s] * valids  # Masking invalid moves\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s  # Renormalize\n",
    "            else:\n",
    "                # If all valid moves were masked make all valid moves equally probable\n",
    "                # NB! All valid moves may be masked if either your NNet architecture is\n",
    "                # insufficient or you've get overfitting or something else.\n",
    "                # If you have got dozens or hundreds of these messages you should\n",
    "                # pay attention to your NNet and/or training process.\n",
    "                log = logging.getLogger(__name__)\n",
    "                log.error(\"All valid moves were masked, doing a workaround.\")\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "            self.Vs[s] = valids\n",
    "            self.Ns[s] = 0\n",
    "\n",
    "            return -v\n",
    "\n",
    "        valids = self.Vs[s]\n",
    "        cur_best = -float(\"inf\")\n",
    "        best_act = -1\n",
    "\n",
    "        # Pick the action with the highest upper confidence bound\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                if (s, a) in self.Qsa:\n",
    "                    u = self.Qsa[(s, a)] + self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s]) / (\n",
    "                        1 + self.Nsa[(s, a)]\n",
    "                    )\n",
    "                else:\n",
    "                    u = self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s] + 1e-8)\n",
    "\n",
    "                if u > cur_best:\n",
    "                    cur_best = u\n",
    "                    best_act = a\n",
    "\n",
    "        a = best_act\n",
    "        next_s, next_player = self.game.getNextState(canonicalBoard, 1, a)\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_player)\n",
    "\n",
    "        v = self.search(next_s)\n",
    "\n",
    "        if (s, a) in self.Qsa:\n",
    "            self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n",
    "            self.Nsa[(s, a)] += 1\n",
    "\n",
    "        else:\n",
    "            self.Qsa[(s, a)] = v\n",
    "            self.Nsa[(s, a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        return -v\n",
    "\n",
    "    def getNsa(self):\n",
    "        return self.Nsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use MCTS to play games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class MonteCarloTreeSearchBasedPlayer():\n",
    "\n",
    "  def __init__(self, game, nnet, args):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: OthelloGame instance\n",
    "        Instance of the OthelloGame class above;\n",
    "      nnet: OthelloNet instance\n",
    "        Instance of the OthelloNNet class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.nnet = nnet\n",
    "    self.args = args\n",
    "    self.mcts = MCTS(game, nnet, args)\n",
    "\n",
    "  def play(self, canonicalBoard, temp=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      canonicalBoard: np.ndarray\n",
    "        Canonical Board of size n x n [6x6 in this case]\n",
    "      temp: Integer\n",
    "        Signifies if game is in terminal state\n",
    "\n",
    "    Returns:\n",
    "      List of probabilities for all actions if temp is 0\n",
    "      Best action based on max probability otherwise\n",
    "    \"\"\"\n",
    "    for i in range(self.args.numMCTSSims):\n",
    "      self.mcts.search(canonicalBoard)\n",
    "\n",
    "    s = self.game.stringRepresentation(canonicalBoard)\n",
    "    self.Nsa = self.mcts.getNsa()\n",
    "    self.counts = [self.Nsa[(s, a)] if (s, a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
    "\n",
    "    if temp == 0:\n",
    "      bestAs = np.array(np.argwhere(self.counts == np.max(self.counts))).flatten()\n",
    "      bestA = np.random.choice(bestAs)\n",
    "      probs = [0] * len(self.counts)\n",
    "      probs[bestA] = 1\n",
    "      return probs\n",
    "\n",
    "    self.counts = [x ** (1. / temp) for x in self.counts]\n",
    "    self.counts_sum = float(sum(self.counts))\n",
    "    probs = [x / self.counts_sum for x in self.counts]\n",
    "    return np.argmax(probs)\n",
    "\n",
    "  def getActionProb(self, canonicalBoard, temp=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      canonicalBoard: np.ndarray\n",
    "        Canonical Board of size n x n [6x6 in this case]\n",
    "      temp: Integer\n",
    "        Signifies if game is in terminal state\n",
    "\n",
    "    Returns:\n",
    "      action_probs: List\n",
    "        Probability associated with corresponding action\n",
    "    \"\"\"\n",
    "    action_probs = np.zeros((self.game.getActionSize()))\n",
    "    best_action = self.play(canonicalBoard)\n",
    "    action_probs[best_action] = 1\n",
    "\n",
    "    return action_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MCTS player against random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2023 has been set.\n",
      "\n",
      "******MCTS player versus random player******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Arena.playGames (1): 100%|██████████| 10/10 [00:15<00:00,  1.58s/it]\n",
      "Arena.playGames (2): 100%|██████████| 10/10 [00:15<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of games won by player1 = 19, number of games won by player2 = 1, out of 20 games\n",
      "\n",
      "Win rate for player1 over 20 games: 95.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load MCTS model from the repository\n",
    "mcts_model_save_name = 'MCTS.pth.tar'\n",
    "path = \"../nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "rp = RandomPlayer(game).play  # All players\n",
    "num_games = 20  # Games\n",
    "n1 = NNet(game)  # nnet players\n",
    "n1.load_checkpoint(folder=path, filename=mcts_model_save_name)\n",
    "args1 = dotdict({'numMCTSSims': 50, 'cpuct':1.0})\n",
    "\n",
    "print('\\n******MCTS player versus random player******')\n",
    "mcts1 = MonteCarloTreeSearchBasedPlayer(game, n1, args1)\n",
    "n1p = lambda x: np.argmax(mcts1.getActionProb(x, temp=0))\n",
    "arena = Arena.Arena(n1p, rp, game, display=OthelloGame.display)\n",
    "MCTS_result = arena.playGames(num_games, verbose=False)\n",
    "print(f\"\\nNumber of games won by player1 = {MCTS_result[0]}, \"\n",
    "      f\"number of games won by player2 = {MCTS_result[1]}, out of {num_games} games\")\n",
    "win_rate_player1 = MCTS_result[0]/num_games\n",
    "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2023 has been set.\n",
      "Random seed 2023 has been set.\n"
     ]
    }
   ],
   "source": [
    "# @title Load in trained value and policy networks\n",
    "model_save_name = 'ValueNetwork.pth.tar'\n",
    "path = \"../nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "vnet = ValueNetwork(game)\n",
    "vnet.load_checkpoint(folder=path, filename=model_save_name)\n",
    "\n",
    "model_save_name = 'PolicyNetwork.pth.tar'\n",
    "path = \"../nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "pnet = PolicyNetwork(game)\n",
    "pnet.load_checkpoint(folder=path, filename=model_save_name)\n",
    "\n",
    "# Alternative if the downloading of trained model didn't work (will train the model)\n",
    "if not os.listdir('../nma_rl_games/alpha-zero/pretrained_models/models/'):\n",
    "  path = \"../nma_rl_games/alpha-zero/pretrained_models/data/\"\n",
    "  loaded_games = loadTrainExamples(folder=path, filename='checkpoint_1.pth.tar')\n",
    "\n",
    "  set_seed(seed=SEED)\n",
    "  game = OthelloGame(6)\n",
    "  vnet = ValueNetwork(game)\n",
    "  vnet.train(loaded_games)\n",
    "\n",
    "  set_seed(seed=SEED)\n",
    "  game = OthelloGame(6)\n",
    "  pnet = PolicyNetwork(game)\n",
    "  pnet.train(loaded_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MCTS player against Value-based player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******MCTS player versus value-based player******\n",
      "Random seed 2023 has been set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Arena.playGames (1): 100%|██████████| 10/10 [00:18<00:00,  1.81s/it]\n",
      "Arena.playGames (2): 100%|██████████| 10/10 [00:17<00:00,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of games won by player1 = 17, number of games won by player2 = 3, out of 20 games\n",
      "\n",
      "Win rate for player1 over 20 games: 85.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n******MCTS player versus value-based player******')\n",
    "set_seed(seed=SEED)\n",
    "vp = ValueBasedPlayer(game, vnet).play  # Value-based player\n",
    "arena = Arena.Arena(n1p, vp, game, display=OthelloGame.display)\n",
    "MC_result = arena.playGames(num_games, verbose=False)\n",
    "\n",
    "print(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n",
    "      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\n",
    "win_rate_player1 = MC_result[0]/num_games\n",
    "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MCTS player against Policy-based player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******MCTS player versus policy-based player******\n",
      "Random seed 2023 has been set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Arena.playGames (1): 100%|██████████| 10/10 [00:17<00:00,  1.76s/it]\n",
      "Arena.playGames (2): 100%|██████████| 10/10 [00:15<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of games won by player1 = 20, number of games won by player2 = 0, out of 20 games\n",
      "\n",
      "Win rate for player1 over 20 games: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n******MCTS player versus policy-based player******')\n",
    "set_seed(seed=SEED)\n",
    "pp = PolicyBasedPlayer(game, pnet).play  # Policy-based player\n",
    "arena = Arena.Arena(n1p, pp, game, display=OthelloGame.display)\n",
    "MC_result = arena.playGames(num_games, verbose=False)\n",
    "\n",
    "print(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n",
    "      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\n",
    "win_rate_player1 = MC_result[0]/num_games\n",
    "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Training Loop in AlphaZero\n",
    "\n",
    "How do we train our networks?\n",
    "\n",
    "- **Self-Play**: AI plays games against itself using MCTS to explore actions.\n",
    "\n",
    "- **MCTS Guidance**: The MCTS outputs a policy (action probabilities) for the root node.\n",
    "  - Store root nodes and corresponding action probabilities and game outcome in a dataset.\n",
    "\n",
    "- **Policy Network**: Trains to predict the action probabilities at the MCTS root node.\n",
    "\n",
    "- **Value Network**: Trains to predict the outcome of the game from the MCTS root node.\n",
    "\n",
    "  - 1 for win, -1 for loss, 0 for draw.\n",
    "  \n",
    "- **Training Update**: After a number of games, the neural networks are updated using the data collected from self-play.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "rise": {
   "auto_select": "none",
   "autolaunch": false,
   "enable_chalkboard": true,
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
